{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984d7a11-21b4-4c0c-aa08-948cc7f3b969",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "## Foundations of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d8e09-cd5b-4971-af2a-841f086d49f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283da74-f7ad-417b-b520-54ced151e4f8",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "- $k$NN and $k$MC illustrate the distinctions between regression and classification, and supervised and unsupervised learning\n",
    "- In both cases, the number of parameters available for fitting the model is really limited --- just $k$ --- and they offer almost no explanation of their results\n",
    "- Today we introduce *linear model*, which optimally weight the explanatory variables in order to predict the outcome variable\n",
    "- These are extremely powerful and easily interpreted tools\n",
    "- There are a lot of ideas and math in these notes: What's important is that you get exposed to ideas, and get the essential ideas of regression\n",
    "- You can spend the rest of your life studying regression models (general linear models, quantile regression, kernel regression, etc.): This is an entry-level discussion that focuses on prediction and the intuition/mechanics of Ordinary Least Squares\n",
    "- more inferential field, looking for causality, one variable is dependent on another, inference cares about how you get to the answer but predictive just cares about the answer \n",
    "- constrained in a way, simplified in a certain extent, spend the rest of your life in regresion model and base, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e980e76-2d58-4830-a125-c00f2183be73",
   "metadata": {},
   "source": [
    "## Vector Multiplication\n",
    "- Suppose we have two vectors $x=(x_1,x_2,...x_K)$ and $b=(b_1,b_2,...,b_K)$ of equal length, $K$\n",
    "- The *dot product* or *inner product* is: \n",
    "\n",
    "$$x_1 b_1 + x_2 b_2 + ... + x_K b_K$$\n",
    "    \n",
    "\n",
    "\n",
    "So we multiply the first two entries together, the second two together, and so on, then sum all the terms.\n",
    "- Common notation for this is:\n",
    "$$\n",
    "x \\cdot b = x^\\top b = x'b = \\langle x, b \\rangle = \\sum_{k=1}^K x_k b_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a411f9",
   "metadata": {},
   "source": [
    "#### if we have two lines in the same direction, they are highly correlated; if they are perpendicular (orthogonal) they are not correlated because move difference direction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdc26d-0a09-486e-87b2-78203e394aa2",
   "metadata": {},
   "source": [
    "## Dot Product and Covariance\n",
    "- Where have you seen something like this before?\n",
    "$$ \\text{cov}(x,y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y}) = \\dfrac{(x-\\bar{x})\\cdot(y-\\bar{y})}{N}$$\n",
    "- The covariance is the statistical version of the dot product, centered around the means and scaled to sample size\n",
    "- The dot product is a general linear algebraic operator, but appears naturally in many places through statistics and computer science, which is why it's worth recognizing and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd64ef6-c510-427b-a8f4-7b58c38d712f",
   "metadata": {},
   "source": [
    "## Dot Product Example\n",
    "- Suppose `y = (3,-5,7)` and `x = (2,4,-6)`. How do we compute the dot product in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a4ad8b-a024-4969-accd-9aeb488d0d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-56\n",
      "-56 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = (3,-5,7)\n",
    "x = (2,4,-6)\n",
    "\n",
    "print( x[0]*y[0] + x[1]*y[1] + x[2]*y[2] )\n",
    "\n",
    "print( np.inner(x,y), '\\n') # Using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f841e3-60c7-47f6-bffc-f5fdfcbae0df",
   "metadata": {},
   "source": [
    "## Angle and Correlation [math]\n",
    "- What \"is\" this thing? It is related to the angle between $x$ and $b$:\n",
    "$$\n",
    "\\cos(\\theta_{xb}) = \\dfrac{x \\cdot b}{\\sqrt{x \\cdot x} \\  \\sqrt{b \\cdot b}}\n",
    "$$\n",
    "The dot product is the mathematical object that creates *angles* between objects in a space.\n",
    "- Recall, $\\cos( 90^\\circ) = \\cos(\\pi/2) = 0$, so if the dot product between two vectors is zero, they are at a \"right angle\" to one another, or **orthogonal**\n",
    "- Where does this show up in statistics? Define $z_x = x - \\bar{x}$ and $z_y = y - \\bar{y}$. Then the correlation is\n",
    "\n",
    "\\begin{alignat*}{2}\n",
    "\\cos(\\theta_{z_x, z_y}) &=& \\dfrac{z_x \\cdot z_y}{\\sqrt{z_x \\cdot z_x} \\  \\sqrt{z_y \\cdot z_y}} \\\\\n",
    "&=& \\dfrac{\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\bar{y})^2} }\\\\\n",
    "&=& \\dfrac{\\dfrac{1}{N}\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{ \\dfrac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^2}\\sqrt{\\dfrac{1}{N}\\sum_{i=1}^N (y_i - \\bar{y})^2} }\\\\\n",
    "&=& \\dfrac{ \\text{cov}(x,y)}{\\sigma_x \\sigma_y}\\\\\n",
    "&=& \\text{cor}(x,y)\n",
    "\\end{alignat*}\n",
    "\n",
    "- What does this mean? The sample correlation between two variables is really the angle between them.\n",
    "- If $\\text{cor}(x,y)=0$ it means the two variables are **independent** or **orthogonal** to one another.\n",
    "- The correlation is at the heart of linear regression: Correlations between variables are roughly what regression coefficients are (namely, $\\text{cov}(x,y)/\\sigma_x^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16c2d8-9fdf-45cf-85bc-d4c0ff9ddb7b",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "- If you stack rows of observations, you can multiply them all at once as follows:\n",
    "$$\n",
    "X \\cdot b =  \\left[\\begin{array}{cccc} x_{11} & x_{12} & \\dots & x_{1K} \\\\ x_{21} & x_{22} & \\dots & x_{2K}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{NK} \\end{array} \\right] \\cdot \\left( \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_K \\end{array}\\right) = \\left( \\begin{array}{c} x_1 \\cdot b \\\\ x_2 \\cdot b \\\\ \\vdots \\\\ x_N \\cdot b\\end{array} \\right)\n",
    "$$\n",
    "- This is part of the motivation for \"clean\"/\"tidy\" data: We do calculations directly on the data frame. Having `NA`'s or ambiguity about what a row or column causes calculations to break down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89608e0f-2946-4268-9217-cbe26cb69a3b",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Example\n",
    "- Suppose\n",
    "$$ X = \\left[ \\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{array} \\right] $$\n",
    "and\n",
    "$$ b = \\left( \\begin{array}{c} 2 \\\\ 4 \\\\ 6 \\end{array} \\right)$$\n",
    "- What is $X \\cdot b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67177551-5701-42f2-b87b-5af6104c20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Vector b:\n",
      "[2 4 6]\n",
      "X dot b:\n",
      "[[ 28  64 100]]\n",
      "X dot b:\n",
      "[[ 28  64 100]]\n"
     ]
    }
   ],
   "source": [
    "X = np.matrix('1,2,3;4,5,6;7,8,9') # Making a matrix in numpy\n",
    "print('Matrix X:')\n",
    "print(X)\n",
    "b = np.array([2,4,6]) # Making a vector in numpy\n",
    "print('Vector b:')\n",
    "print(b)\n",
    "y = np.matmul(X,b) # Matrix multiplication in numpy\n",
    "print('X dot b:')\n",
    "print(y)\n",
    "y = X @ b # Operator notation\n",
    "print('X dot b:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de37e7-0f4c-47c5-9977-8ce9b79137ae",
   "metadata": {},
   "source": [
    "## Linear Models: Setup\n",
    "- The data include an $N \\times K$ data matrix $X$ with $N$ observations and $K$ variables, and an $N$-length vector of outcomes, $y$\n",
    "- We wish to use $X$ to explain $y$\n",
    "- In particular, we want to explain $y$ by $X$ with a linear model,\n",
    "$$\n",
    "y = X \\cdot b\n",
    "$$\n",
    "where $b$ is a $K$-length vector of *coefficients* or *weights*\n",
    "- So for $b = (b_1, b_2, ..., b_K)$, variable $k$ is multiplied by $b_k$ to scale its contribution to the prediction of $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c297bd",
   "metadata": {},
   "source": [
    "### coefficient number is the covariance between that number and the matrix; b is the coefficient number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f570c-921c-4441-8513-825ceaf81023",
   "metadata": {},
   "source": [
    "## Linear Models: Prediction\n",
    "- To make a prediction for a new values $\\hat{x} = (\\hat{x}_1, \\hat{x}_2, ..., \\hat{x}_L)$, we compute the dot product:\n",
    "$$\n",
    "\\hat{y} = \\hat{x} \\cdot b = \\hat{x}_1 b_1  + \\hat{x}_2 b_2  + ... + \\hat{x}_K b_K \n",
    "$$\n",
    "- The prediction is the straightforward part: Picking the weights $b$ is the hard part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba229cbd",
   "metadata": {},
   "source": [
    "### coefficient is zero when there is no corelation; maybe there's a relationship in the opposite relationship, negative relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9466c-74e6-46d5-84b4-71ee078c198e",
   "metadata": {},
   "source": [
    "## Linear Models: Prediction\n",
    "- In terms of matrix multiplication, a fitted model is a $\\hat{b}$, and the predictions are created for data $\\hat{X}$ as\n",
    "$$\n",
    "\\hat{X} \\cdot \\hat{b} =  \\left[\\begin{array}{cccc} \\hat{x}_{11} & \\hat{x}_{12} & \\dots & \\hat{x}_{1K} \\\\ \\hat{x}_{21} & \\hat{x}_{22} & \\dots & \\hat{x}_{2K}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat{x}_{N1} & \\hat{x}_{N2} & \\dots & \\hat{x}_{NK} \\end{array} \\right] \\left( \\begin{array}{c} \\hat{b}_1 \\\\ \\hat{b}_2 \\\\ \\vdots \\\\ \\hat{b}_K \\end{array}\\right) = \\left( \\begin{array}{c} \\hat{x}_1 \\cdot \\hat{b} \\\\ \\hat{x}_2 \\cdot \\hat{b} \\\\ \\vdots \\\\ \\hat{x}_N \\cdot \\hat{b}\\end{array} \\right) = \\left( \\begin{array}{c} \\hat{y}_1 \\\\ \\hat{y}_2  \\\\ \\vdots \\\\ \\hat{y}_N \\end{array} \\right)\n",
    "$$\n",
    "- On a compute, these kinds of calulations are fast and efficient, and hardware like GPUs vastly speed up matrix/dot product calculations. Even for very complex models, a linear relationship between variables often appears somewhere (neural networks are non-linear compositions/nests of linear models).\n",
    "- The next chunk of code gives a visual example of what we're talking about, for a simple model $\\hat{y} = b_0 + b_1 x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07654f1b-b460-4afc-b398-568a09995641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(500) # Set the seed for the random number generator\n",
    "N = 30\n",
    "\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise, random noise, goal is to identify contribution of x and explaining y even when there is nosie; adding noise to x and predict y\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps #what y interecept equals when all the coefficients are zeros, where the equation starts\n",
    "\n",
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    b1 = np.inner(x-x_bar,y-y_bar)/np.inner(x-x_bar,x) #coefficient value; a ratio, amount of change in the explainer (denominator) and add in the amount of change in y (numerator) and see the change\n",
    "    b0 = y_bar - b1*x_bar #intercept coefficient\n",
    "    y_hat = b0 + b1*x\n",
    "    residuals = y - y_hat\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})\n",
    "    \n",
    "reg = slr(x,y) # Run the regression\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315757e6",
   "metadata": {},
   "source": [
    "## ^^ goal here is to try to opptimize, the best predictor, give me the middle point of the distribution of y data as it relates to x; calculate how far off are our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43906235-4b16-489c-9708-44102420b202",
   "metadata": {},
   "source": [
    "## Case Study: Car Prices\n",
    "- How does car age predict price?\n",
    "- I'm going to interactively look at the data, clean outliers, take transformations, and regress, somewhat sloppily, then show you what the picture looks like if you don't do these steps\n",
    "- This is a practical set of steps to take a\n",
    "- My rough advice: Linear regressions is defined to approximate the **conditional expectation function** (CEF), $\\mathbb{E}[y|x]$. It is \"working\" if your line is tracking with the average value of $y$ as $x$ varies, and is failing if the line is way off the mark (typically due to 1. outliers or 2. non-linearity of the CEF in $x$, requiring further transformation of the variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4104fd15-0c52-4713-91ac-5b1c90cc55e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/cars_hw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/cars_hw.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df0 \u001b[38;5;241m=\u001b[39m df \u001b[38;5;66;03m# Let's keep the original data around for comparison purposes\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cars_hw.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('./data/cars_hw.csv') # Load the data\n",
    "df0 = df # Let's keep the original data around for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61152a3-2bc3-42a8-a5cb-edc6ae39c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Glance at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a478d4-b9a9-49c7-bcc9-b00319959a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].hist(bins=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a753b8-c08f-497e-abde-c15554ef31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = max(df['year'])-df['year'] # Convert year to age\n",
    "df['age'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff889623-314b-4640-b36c-ec3a39ec4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price',x='age') # We've got some outliers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48850681-19a0-494f-9ee5-52c2db7fd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take arcsinh transformation to rescale the variables\n",
    "df['price_ihs'] = np.arcsinh(df['price'])\n",
    "df['age_ihs'] = np.arcsinh(df['age'])\n",
    "sns.scatterplot(data=df,y='price_ihs',x='age_ihs') # We've got some outliers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a853c-7bbe-4605-b246-99b6b5cee447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_ihs'].plot.box() # Outliers below 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9793e-4046-47bd-81dd-2e4a3f10e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_ihs'].plot.box() # Outliers above 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b73b6-1548-4d77-8c54-9703d99288a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers:\n",
    "df = df.loc[df['price_ihs']>9,:]\n",
    "df = df.loc[df['age_ihs']<4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e3770-ff1a-4bb0-af11-7c2efd7be3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price_ihs',x='age_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679189f7-9a77-47bb-89c1-3c830fe626f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['age_ihs']\n",
    "y = df['price_ihs']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression: Looks OK')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782edfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('age',inplace=True) # Sorts the data by age of car\n",
    "\n",
    "x = df['age']\n",
    "y = df['price']\n",
    "coef = slr( df['age_ihs'], df['price_ihs'])\n",
    "y_hat = np.sinh(coef['b0']+coef['b1']*df['age_ihs']) # sinh is the inverse of arcsinh\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black',linewidth=2)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression: Original Units')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe215-2b2f-458f-8f23-3a81aa3ec47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why bother with all that data cleaning?\n",
    "x = df0['age']\n",
    "y = df0['price']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.title('Linear Regression: Looks Bad')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e0705-8e75-43ab-8cf2-c7717a333520",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- Predict car prices from attributes\n",
    "- Predict Airbnb rental prices from housing features\n",
    "- Predict probability of heart failure from patient characteristics\n",
    "- Predict bond and sentence from defendant demographics and criminal record\n",
    "- Predict student loan debt creation as a function of institution characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86037902-98c1-45a7-b4f5-e522eb12af28",
   "metadata": {},
   "source": [
    "## Errors/Residuals\n",
    "- Fitting a linear model is based on minimizing the unexplained variation in the data\n",
    "- Let $\\hat{y}_i = x_i \\cdot b$ be the prediction for observation $i$\n",
    "- The *residual* or *error* for observation $i$ is\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i = y_i - x_i \\cdot b \n",
    "$$\n",
    "This is how far off the in-sample prediction is, using the coefficients $b$ and variables $x_i$ for observation $i$ to make a prediction $\\hat{y}_i$ --- how bad is our model at predicting values for data we already have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f57042-dbdb-44c2-aa6b-706d66271121",
   "metadata": {},
   "source": [
    "## Sum-of-Squared-Error, `SSE`\n",
    "- Some errors will generally be positive and some negative, but we want to count any error as undesirable, and larger errors as even worse failures. So, we square the error,\n",
    "$$\n",
    "e_i^2 = (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "and sum over the observations,\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^N e_i^2 = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "to get the **Sum of Squared Error**. \n",
    "- This is often normalized as an average, to get the **mean squared error**,\n",
    "$$\n",
    "\\text{MSE} = \\dfrac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2,\n",
    "$$\n",
    "and often further normalized by taking the square root to get the **root mean square error**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\dfrac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2}.\n",
    "$$\n",
    "- From a stats perspective there are subtle differences, but from a model-fitting perspective, these are all fundamentally the same thing: A metric of model performance based on squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86784247-2172-48e0-a05e-7b518d5ce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of the squares of the red lines is the SSE:\n",
    "np.random.seed(500) # Set the seed for the random number generator\n",
    "N = 30\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps\n",
    "reg = slr(x,y) # Run the regression\n",
    "for i in range(len(x)):\n",
    "    plt.vlines(x[i], y[i], reg['y_hat'][i], color='r') # Visualize residuals\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Visualizing SSE: Sum of Squared Red Lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06cd3e-6ee9-41b6-9239-b91429c502d0",
   "metadata": {},
   "source": [
    "## Other Model Metrics\n",
    "- There is no reason you can't target other measures of model performance, like **mean absolute deviation** which is more robust to outliers,\n",
    "$$ \\text{MAD} = \\dfrac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|$$\n",
    "or **worst absolute deviation** where worst-case prediction is the concern,\n",
    "$$ \\text{WAD} = \\max_i |y_i - \\hat{y}_i|$$\n",
    "- There are (at least) hundreds of different metrics of model success besides `SSE`\n",
    "- Why `SSE`? Probably because we can easily use calculus to minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6183b-5a25-437a-91b4-52d58dce2d8b",
   "metadata": {},
   "source": [
    "## Minimizing the `SSE`\n",
    "- The goal of *linear regression* is to pick $b$ to make $\\text{SSE}(b)$ as small as possible. \n",
    "- Let's do this for a *single linear model* with a constant and one explanatory/feature variable $x_i$:\n",
    "$$\n",
    "y_i = b_0 \\times 1 + b_1 \\times x_i\n",
    "$$\n",
    "- Then the `SSE` is:\n",
    "$$\n",
    "\\text{SSE}(b_0, b_1) = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75755795-3377-42ea-8dd5-508c1501b789",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- From my perpsective, the main reason to study calculus is to learn how to minimize/maximize and approximate functions\n",
    "- To do interesting things in quantitative subjects, you typically need the mathematical background to maximize or minimize things like $\\text{SSE}$ with respsect to $b_0$ and $b_1$\n",
    "- Roughly, to minimize a function $f(b)$, you take the derivative with respect to $b$, $f'(b)$, set it equal to zero, and solve for $b^*$ --- this is called a *first order necessary condition*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57bf370-6bea-4cf7-be5c-2bdf3b4fa415",
   "metadata": {},
   "source": [
    "## First-Order Conditions for Optimization\n",
    "- If $\\text{SSE}(b_0,b_1) = \\sum_{i=1}^N(y_i-b_0 - b_1x_i)^2$, the necessary condition for $b_0$ is\n",
    "$$\n",
    "\\sum_{i=1}^N-2(y_i-b_0-b_1 x_i) = 0\n",
    "$$\n",
    "and the necessary condition for $b_1$ is\n",
    "$$\n",
    "\\sum_{i=1}^N -2(y_i - b_0 - b_1 x_i)x_i = 0\n",
    "$$\n",
    "- How do we simplify these?\n",
    "- Define $\\bar{x}$ as the mean of $x$,\n",
    "$$\n",
    "\\bar{x} = \\dfrac{1}{N} \\sum_{i=1}^N x_i,\n",
    "$$\n",
    "and similarly for $\\bar{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb27a0d-d526-4cf3-94c4-5f6b8da62ede",
   "metadata": {},
   "source": [
    "## The First Condition\n",
    "- We can simplify the first condition for $b_0$ as:\n",
    "\\begin{alignat*}{2}\n",
    "0 &=& \\sum_{i=1}^N(y_i-b_0-b_1 x_i) \\\\\n",
    "&=&  \\sum_{i=1}^N y_i- \\sum_{i=1}^N b_0-b_1 \\sum_{i=1}^N x_i  \\quad (\\text{Distribute summation})\\\\\n",
    "&=& \\sum_{i=1}^N y_i- N b_0-b_1 \\sum_{i=1}^N x_i \\quad (\\text{Sum $b_0$ $N$ times})\\\\\n",
    "&=& \\dfrac{\\sum_{i=1}^N y_i}{N} -  b_0-b_1 \\dfrac{\\sum_{i=1}^N x_i}{N} \\quad (\\text{Divide by $N$})\\\\\n",
    "0 &=& \\bar{y} - b_0 - b_1 \\bar{x} \\quad (\\text{Use definitions})\n",
    "\\end{alignat*}\n",
    "implying $b_0^* = \\bar{y} - b_1^* \\bar{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0312382-89a6-45c0-b633-5c6148c0bfe9",
   "metadata": {},
   "source": [
    "## The Second Condition\n",
    "- For the second condition for $b_1$:\n",
    "\\begin{alignat*}{2}\n",
    "0 &=& \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)x_i \\\\\n",
    "&=& \\sum_{i=1}^N (y_i - (\\bar{y} - b_1 \\bar{x}) - b_1 x_i)x_i \\quad (\\text{Substitute in $b_0^*$})\\\\\n",
    "&=& \\sum_{i=1}^N (y_i - \\bar{y})x_i - b_1 (x_i -\\bar{x})x_i \\quad (\\text{Distribute $x_i$, group terms})\\\\\n",
    "0 &=& \\sum_{i=1}^N (y_i - \\bar{y})x_i - b_1 \\sum_{i=1}^N (x_i -\\bar{x})x_i \\quad (\\text{Distribute summation})\\\\\n",
    "\\end{alignat*}\n",
    "implying\n",
    "$$\n",
    "b_1^* = \\dfrac{\\sum_{i=1}^N (y_i - \\bar{y})x_i}{\\sum_{i=1}^N (x_i -\\bar{x})x_i}.\n",
    "$$\n",
    "(This is roughly the correlation between $x$ and $y$ divided by the variance of $x$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2212f8e-81b8-4e4b-a25e-c12063dd5b88",
   "metadata": {},
   "source": [
    "## Single Linear Regression \n",
    "- Notice that the first condition can be written as\n",
    "$$\n",
    "0 = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i) \n",
    "  = \\sum_{i=1}^N (y_i - \\hat{y}_i)\n",
    " = \\dfrac{1}{N} \\sum_{i=1}^N e_i \n",
    "$$\n",
    "so **the average error is equal to zero at the optimum**\n",
    "- The second condition can be written as\n",
    "$$\n",
    "0 = \\sum_{i=1}^N (y_i - b_0 - b_1 x_i)x_i \n",
    "= \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i \n",
    " = \\sum_{i=1}^N e_i x_i \n",
    " = e \\cdot x \n",
    "$$\n",
    "**The error term and explanatory variable are statistically uncorrelated, and at \"right angles\" to one another (orthogonal)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209058c-0eba-46dd-9203-618ed742be83",
   "metadata": {},
   "source": [
    "## Single Linear Regression Function\n",
    "- Here is an implementation of single linear regression, which returns a dictionary including the coefficients, the predicted values, and the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d802341-4d04-4b2e-bfc1-b34d1ee22a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x) # Average of x's\n",
    "    y_bar = np.mean(y) # Average of y's\n",
    "    b1 = ((x-x_bar)@(y-y_bar))/((x-x_bar)@x) # Slope coefficient\n",
    "    b0 = y_bar - b1*x_bar # Intercept coefficient\n",
    "    y_hat = b0 + b1*x   # Compute predictions\n",
    "    residuals = y - y_hat   # Compute residuals\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a14730-5a45-4253-a6b3-0adf822baa0f",
   "metadata": {},
   "source": [
    "## Partialing Out, Projection\n",
    "- The regression breaks $y$ into two pieces:\n",
    "\\begin{alignat*}{2}\n",
    "y_i &=& (y_i - \\hat{y}_i) + \\hat{y}_i\\\\\n",
    "&=& e_i + \\hat{y}_i\\\\\n",
    "\\underbrace{y_i}_{\\text{True value}} &=& \\underbrace{e_i}_{\\text{Error, residual}} + \\underbrace{x_i \\cdot b}_{\\text{Model, prediction}}\n",
    "\\end{alignat*}\n",
    "- But the residual from OLS averages to zero: It is uncorrelated with the prediction\n",
    "- You can understand linear regression as removing the variation in $Y$ that can be explained by $X$ --- The residual contains all of the noise, the predictor $\\hat{b} \\cdot x$ contains all of the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80dddc-bed9-4599-9449-d486cbd4123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(102) # Set the seed for the random number generator\n",
    "N = 30\n",
    "\n",
    "x = 5 + 2*np.random.normal(0,1,size = N) # Create an x\n",
    "eps = np.random.normal(0,3,size = N) # Create noise\n",
    "b0 = -1 # Intercept coefficoent\n",
    "b1 = 3 # Slope coefficient\n",
    "y = b0 + b1*x + eps\n",
    "\n",
    "def slr(x,y): # Single Linear Regression Function\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    b1 = np.inner(x-x_bar,y-y_bar)/np.inner(x-x_bar,x)\n",
    "    b0 = y_bar - b1*x_bar\n",
    "    y_hat = b0 + b1*x\n",
    "    residuals = y - y_hat\n",
    "    return({'b0':b0,'b1':b1,'y_hat':y_hat,'residuals':residuals})\n",
    "    \n",
    "reg = slr(x,y) # Run the regression\n",
    "print('Coefficients: ',reg['b0'],reg['b1'])\n",
    "\n",
    "# Plot the resuts:\n",
    "for i in range(len(x)):\n",
    "    plt.vlines(x[i], y[i], reg['y_hat'][i], color='r') # Visualize residuals\n",
    "\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,reg['y_hat'],label='Regression Line')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Linear Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b18e9-1af2-4cfa-bdd5-96140510a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x,reg['y_hat'])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y_hat\")\n",
    "plt.title('Prediction: Signal')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x,reg['residuals'])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"e\")\n",
    "plt.title('Residuals: Noise')\n",
    "\n",
    "pd.DataFrame({'residuals':reg['residuals'],'x':x}).cov() # Compute correlation between e and x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d46167-0b5e-4aff-9747-0a6e52ce8977",
   "metadata": {},
   "source": [
    "## Another SLR Example\n",
    "- Now that we understand SLR a bit better, let's do another case study, of Price versus Mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b963c2-1888-4407-99b1-adc7a6f4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0 # Let's start over\n",
    "df['mileage'].describe()\n",
    "df['mileage_ihs'] = np.arcsinh(df['mileage'])\n",
    "df['price_ihs'] = np.arcsinh(df['price'])\n",
    "df.loc[:,['mileage_ihs','price_ihs']].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc228-fa83-4c88-bdf3-5704c816e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,y='price_ihs',x='mileage_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983782c-04a2-4166-9be9-d7c29510cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df,y='price_ihs',x='mileage_ihs',kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfe717-d5a7-4845-bd3c-3327f423cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers:\n",
    "df = df.loc[df['mileage_ihs']>9,:] \n",
    "df = df.loc[df['mileage_ihs']<13,:]\n",
    "df = df.loc[df['price_ihs']>9,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c80f2-b732-4b55-a828-c43f275dd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df,x='price_ihs',y='mileage_ihs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c1847-1c1e-423e-b596-4dfad7b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['mileage_ihs']\n",
    "y = df['price_ihs']\n",
    "\n",
    "coef = slr(x,y) # Single Linear Regression\n",
    "print('Intercept: ',coef['b0'], '\\n', ' Slope: ', coef['b1']) \n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x # Compute predictions\n",
    "\n",
    "# Scatter plot of fit:\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"Mileage, ihs\")\n",
    "plt.ylabel(\"Price, ihs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('mileage',inplace=True) # Sorts the data by age of car\n",
    "\n",
    "x = df['mileage']\n",
    "y = df['price']\n",
    "coef = slr( df['mileage_ihs'], df['price_ihs'])\n",
    "y_hat = np.sinh(coef['b0']+coef['b1']*df['mileage_ihs']) # sinh is the inverse of arcsinh\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black',linewidth=2)\n",
    "plt.xlabel(\"Mileage\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression: Original Units')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fa87b-45a6-475b-b050-f4ca0314c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without cleaning/feature engineering:\n",
    "x = df0['mileage']\n",
    "y = df0['price']\n",
    "\n",
    "coef = slr(x,y)\n",
    "\n",
    "y_hat = coef['b0']+coef['b1']*x\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x,y_hat,label='Regression Line',color='black')\n",
    "plt.xlabel(\"Mileage\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841f367-6cfa-4b24-8a59-1278ff8dd200",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "- The previous discussion is great: we have some geometic intuition about how linear regression works, its mathematical foundation, and its key properties\n",
    "- Problem: It's only worked out for one regressor/feature/explanatory variable\n",
    "- How do we extend this to multiple variables? i.e., the model\n",
    "$$ y = \\underbrace{b_0}_{\\text{Intercept}} \\times 1 + b_1 \\times  x_1 + b_2 \\times x_2 + ... + b_K \\times x_K $$\n",
    "- We adjust the `SSE` to include all the variables of interest:\n",
    "$$ SSE = \\sum_{i=1}^N (y_i - b_0 - b_1 x_{i1} - b_2 x_{i2} - ... - b_K x_{iK} )^2 = \\sum_{i=1}^N (y_i - X_i \\cdot b)^2$$\n",
    "and maximize over $(b_0, b_1, ..., b_K)$.\n",
    "- We won't go over the details analytically, but minimizing $(y-Xb)'(y-Xb)$ can be done computationally (gradient descent) or using linear algebra (the solution is $b^* = (X'X)^{-1}(X'y)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df0295-459f-497b-9833-5dff8a243b7e",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Example\n",
    "- Before we looked at `age` and `mileage` separately as predictors of price\n",
    "- Now that we have MLR, we can combine then into a single model\n",
    "$$ \\text{price} = b_0 + b_{1} \\times \\text{age} + b_{2}\\times\\text{mileage}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759c896-9023-4db1-9baf-34ce09a5aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-step multilinear regression:\n",
    "def mlr(X,y): # Multiple linear regression, matrix algebra approach\n",
    "    XpX = X.T@X # Compute X'X\n",
    "    Xpy = X.T@y # Compute X'y\n",
    "    b = np.linalg.solve(XpX, Xpy) # Solve normal equations\n",
    "    y_hat = X@b # Compute predictions\n",
    "    residuals = y-y_hat # Compute residuals\n",
    "    SSE =  np.inner(residuals,residuals) # Compute SSE\n",
    "    rsq = 1 - SSE/np.inner( y-np.mean(y),y-np.mean(y)) # Compute Rsq\n",
    "    return({'b':b,'y_hat':y_hat,'residuals':residuals,'rsq':rsq,'SSE':SSE})\n",
    "\n",
    "df['(Intercept)'] = 1\n",
    "X = df.loc[:,['(Intercept)','age_ihs','mileage_ihs'] ]\n",
    "y = df['price_ihs']\n",
    "reg = mlr(X,y)\n",
    "print('MLR coefficients: ', reg['b']) # Same values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fcc0c3-7d22-4b9a-966d-c909ba08cb95",
   "metadata": {},
   "source": [
    "## How does MLR work?\n",
    "- This is the deep idea in regression\n",
    "- Consider the $x_k$ whose coefficient you want to compute:\n",
    "    1. Regress $y$ and $x_k$ on all of the other coefficients, yielding residuals $r_y$ and $r_{x_k}$\n",
    "    2. Now regress $r_y$ on $r_{x_k}$ and a constant\n",
    "- The slope coefficient for the procedure described above is the same as for MLR\n",
    "- What does this mean? Linear regression \"partials out\" all of the variation in $y$ that can be explained by the other features, and the optimal weight $b_k$ reflects only the remaining variation in $y$ that can be explained by $x_k$ alone:\n",
    "$$ \n",
    "b_k = \\dfrac{ \\text{cov}(r_y, r_{x_k})}{\\sigma^2_{r_{x_k}}}\n",
    "$$  \n",
    "- We will return to this idea a lot to explain phenomena related to linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972f8f4-19bb-433c-ba9f-3d01a40511c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do all the steps laboriously:\n",
    "\n",
    "# Pick variables for analysis:\n",
    "y = df['price_ihs']\n",
    "x1 = df['mileage_ihs']\n",
    "x2 = df['age_ihs']\n",
    "\n",
    "reg1_y = slr(x1,y) # Regress y on x1\n",
    "reg1_2 = slr(x1,x2) # Regress x2 on x1\n",
    "y_temp = reg1_y['residuals'] # Extract the residual for y\n",
    "x2_temp = reg1_2['residuals'] # Extract the residual for x2\n",
    "reg_y_x2 = slr(x2_temp,y_temp) # Regress residuals on each other\n",
    "print('Age coefficient: ', reg_y_x2['b1'])\n",
    "\n",
    "reg2_y = slr(x2,y) # Regress y on x2\n",
    "reg2_1 = slr(x2,x1) # Regress x1 on x2\n",
    "y_resid = reg2_y['residuals'] # Extract the residual for y\n",
    "x1_resid = reg2_1['residuals'] # Extract the residual for x1\n",
    "reg_y_x1 = slr(x1_resid,y_resid) # Regress residuals on each other\n",
    "print('Mileage coefficient: ', reg_y_x1['b1'])\n",
    "\n",
    "# Compute intercept:\n",
    "b0 = np.mean(y) - reg_y_x1['b1'] * np.mean(x1) - reg_y_x2['b1']*np.mean(x2)\n",
    "print('Intercept: ', b0, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f48b7-5b46-40ea-9a2a-c3c2ce0854a0",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression \n",
    "- Once you have multiple linear regression, you can build much more complex models of phenomena\n",
    "- We probably want to include $\\text{age}^2$ and $\\text{mileage}^2$ to control for non-linear aging effects:\n",
    "$$ \\text{price} = b_0 + b_{1} \\times \\text{age} + b_{2} \\times\\text{age}^2 + b_{3}\\times\\text{mileage} + b_{4}\\times\\text{mileage}^2 $$\n",
    "- This gives a highly flexible and extensible way of modeling how features predict a target variable; you can only unlock the power of linear regression if you are willing to give it a large feature space to work with\n",
    "- Eventually, we want to discipline this process by using data, not just making up models that run the risk of overfitting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170483-e56c-4e44-b3ca-a7757f69a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b1d65-c6e4-4409-b2e2-ec6fa6a0fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price_ihs']\n",
    "df['mileage_ihs_sq'] = df['mileage_ihs']**2\n",
    "df['age_ihs_sq'] = df['age_ihs']**2\n",
    "df['(Intercept)'] = 1\n",
    "vars = ['(Intercept)','mileage_ihs','mileage_ihs_sq','age_ihs','age_ihs_sq']\n",
    "X = df.loc[:,vars]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f791305-8d64-4385-88be-b60233bfc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = mlr(X,y) # Run multiple linear regression\n",
    "print(reg['b']) # Print coefficients \n",
    "reg['residuals'].plot.kde() # Plot residuals\n",
    "print(reg['rsq']) # R-squared measure of model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e25a47-f01f-4a2c-b29d-b3f6a2aaecf9",
   "metadata": {},
   "source": [
    "## The `sklearn.linear_model` Module\n",
    "- Scikit-Learn has a linear regression object that can be used out-of-the box:\n",
    "    - `from sklearn.linear_model import LinearRegression` to load the linear regression module\n",
    "    - `myRegression = LinearRegression().fit(X, y)` fits \n",
    "- You'll see below that the results very similar to the `mlr` function above: They're using the same NumPy code \"under the hood\" to solve for the OLS coefficients, but with some additional feature engineering/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2109e9a-f770-40b9-b4b5-ec1f4daedc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Import linear regression model\n",
    "\n",
    "vars = ['mileage_ihs','mileage_ihs_sq','age_ihs','age_ihs_sq'] # This is a list of variables to use\n",
    "\n",
    "X = df.loc[:,vars] # Construct data matrix\n",
    "X.head() # Peek at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912b417-981a-4274-8965-a29cddc4a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X, y) # Fit the linear model\n",
    "print(reg.intercept_) # Intercept value\n",
    "print(reg.coef_) # Regression coefficients\n",
    "print(reg.score(X, y)) # R squared measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bbadd-fdf0-4c28-b6fb-7573b35b2cec",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- `./data/USA_cars_datasets.csv`\n",
    "- `./data/airbnb_hw.csv`\n",
    "- `./data/heart_failure/heart_failure_clinical_records_dataset.csv`\n",
    "- `./data/pierce_county_house_sales.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60debe9f-de82-4e99-b625-d8418ac8ea12",
   "metadata": {},
   "source": [
    "## Coefficient of Determination, $R^2$\n",
    "- A natural question then is, how much noise is left to explain? How well does the model fit the data?\n",
    "- There is a common statistic used to summarize how predictive $X$ is of $y$, using the OLS coefficients, called the *coefficient of determination* or $R^2$:\n",
    "$$\n",
    "R^2 = 1 - \\dfrac{\\sum_{i=1}^N (y_i - x_i \\cdot \\hat{b})^2}{\\sum_{i=1}^N(y_i-\\bar{y})^2}\n",
    "$$\n",
    "- That numerator is the sum-of-squared-error, evaluated at $\\hat{b}$: How much error remains after trying to explain it using OLS?\n",
    "- That denominator is the error of predicting $y_i$ with just the mean as the predictor, $\\bar{y}$\n",
    "- So the ratio is the reduction in the `SSE` by using the explanatory variables rather than just the sample mean\n",
    "- This is a nice way to evaluate the model, but should not become an end in itself: Adding more variables always raises the $R^2$, but does not necessarily improve out-of-sample prediction\n",
    "- $R^2$ isn't an end in itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec7a38-eafa-4b80-a5be-7b14b95f7d5b",
   "metadata": {},
   "source": [
    "## What, exactly, makes a model linear?\n",
    "- A model is linear because the coefficients, $b$, enter the prediction as\n",
    "$$\n",
    "\\hat{y} = b \\cdot \\hat{x} = b_1 \\hat{x}_1 + b_2 \\hat{x}_2 + ... b_J \\hat{x}_J\n",
    "$$\n",
    "so that the relationship between $\\hat{y}$ and each $\\hat{x}_j$ is a linear one through $b_j$\n",
    "- Taking non-linear transformations of the $x_j$'s simply gives new variables, like $\\log(x_j)$ or $x_j^2$ -- *Transformations of the regressors don't make the model nonlinear*\n",
    "- Likewise, if there was a coefficient written in a funky nonlinear way, like $\\sqrt{ \\text{asinh}(b_j)}$, you can simply relabel that coefficient as $b_j \\leftarrow \\sqrt{ \\text{asinh}(b_j)}$ and make the model linear again.\n",
    "- There *are* many interesting non-linear models, but they are often derived from a domain-specific theory and require additional work to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febc859-e48d-46b6-b946-f3ad6405663c",
   "metadata": {},
   "source": [
    "## Beyond Fitting the Model\n",
    "- Up to here, the discussion has been a standard analysis of linear models\n",
    "- From the perspective of data science, we have **model specification** questions: What variables should go in the model?\n",
    "- The power of linear models is that you can interact variables and expand the feature space, allowing you to better control how relationships between variables are modelled\n",
    "- The risk is that your model turns to mush (multicolinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb1b63-eca0-4858-9e63-899eed0f7eda",
   "metadata": {},
   "source": [
    "## Expanding the Feature Space\n",
    "- To fully leverage the power of regression, you have many options to expand the range of variables that the model can use to explain the variation in the data:\n",
    "    - An **interaction term** is when you take two explanatory variables, say $x_1$ and $x_2$, and multiply them together to get a new explanatory variable, $z = x_1 x_2$, like $\\text{mileage} \\times \\text{age}$\n",
    "    - A **polynomial family** is when you take an explanatory variable, say $x$, and compute its powers $x^2$, $x^3$, ... , $x^K$. (There are many kinds of families besides polynomial.)\n",
    "    - A **logarithmic transformation** or **inverse hyperbolic sine transformation**\n",
    "    - A **maxmin normalization** or **z-score normalization**\n",
    "    - More advanced tools like **principal components analysis** decomposition of the data or **splines** that create highly transformations of variables\n",
    "    - Any combination of the above\n",
    "- This is where we run into a significant danger of overfitting: The more complex the feature space, the more opportunities we give the model to pick non-representative cases around which to build non-representative/externally invalid models\n",
    "- You can do a lot of these by hand, using `df['new_var'] = transformation(df['old_var'])`, but many common and tedious transformations are already programmed in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d24cd1-2d61-4ed6-97f5-c8d4246f5714",
   "metadata": {},
   "source": [
    "## Quick Polynomial and Interaction Features/Variables\n",
    "- It it tedious to compute interaction terms like $x_1 \\times x_2$ or $x^2, x^3, ..., x^m$ on your own, and it's a very common task, so `sklearn` has a convenient tool for accomplishing this\n",
    "- The `PolynomialFeatures` is the object in `sklearn.preprocessing` that can quickly create matrices of exponenetiated variables for you without doing it yourself\n",
    "- The basic steps are:\n",
    "    1. Import the object: `from sklearn.preprocessing import PolynomialFeatures`\n",
    "    2. Decide the degree of the expansion and create an expander: `expander = PolynomialFeatures(degree=2,include_bias=False)`\n",
    "    3. Execute the transformations: `Z = expander.fit_transform(X)` and get labels for the columns `names = expander.get_feature_names_out()`\n",
    "    4. Create a new dataframe: `zdf = pd.DataFrame(data=Z, columns = names)`\n",
    "    5. Possibly concatenate this new dataframe with other data\n",
    "- Use this power wisely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66991f0-4a98-4275-aed5-0361bfbdebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Example dataframe:\n",
    "df = pd.DataFrame({'apples':np.array([1,3,5]),'joules':np.array([3,-2,1]), 'stocks':np.array([-2,1,4])})\n",
    "X = df\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedb3e4-f3bf-472c-81c9-a17572d682bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expander = PolynomialFeatures(degree=2,include_bias=False) # Create the expander\n",
    "Z = expander.fit_transform(X) # Pass the df into the expander to get powers/interactions of x and y\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "zdf = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "zdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9d97d-08df-4708-ab33-f5a579f08932",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute all polynomials up to degree 3:\n",
    "expander = PolynomialFeatures(degree=3,include_bias=False) # Create the expander\n",
    "Z = expander.fit_transform(X) # Pass the df into the expander to get powers/interactions of x and y\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "zdf = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "zdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0288392-a7bf-40cb-9e5e-0fd73e42be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only compute the interactions up to degree 3:\n",
    "expander = PolynomialFeatures(degree=3,interaction_only=True,include_bias=False) # Create the expander\n",
    "Z = expander.fit_transform(X) # Pass the df into the expander to get powers/interactions of x and y\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "zdf = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "zdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579f7b7-f0e2-47c5-af01-f4529f260553",
   "metadata": {},
   "source": [
    "## Quick One Hot Encoding\n",
    "- We often have categorical data that need to be converted to numerical values for `sklearn` to work\n",
    "- You can use Pandas to create dummy variables out of the categorical variable using `pd.get_dummies(varName)`\n",
    "- You can use sklearn's `LabelBinarizer` from `.preprocessing` to create a matrix of one-hot encoded variables (the `OneHotEncoder` can be more complex to use)\n",
    "- **We rarely interact put one-hot variables into the polynomial feature expander, unless it's a thing we've explicitly decided to do**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4cb4a-b167-40c4-9948-54570f40abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'apples':np.array([1,3,5,7]),\n",
    "                   'joules':np.array([3,-2,1,2]), \n",
    "                   'stocks':np.array([-2,1,4,6]),\n",
    "                   'bird':['Cardinal','Oriole','Robin','Oriole']})\n",
    "X = df\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c9c84-1e05-45fa-b88a-87347caa5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Pandas:\n",
    "ddf = pd.get_dummies(X['bird'])\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b762d-2dd2-4d6b-aad4-b5645135758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Pandas, if you don't like booleans:\n",
    "ddf = pd.get_dummies(X['bird'],dtype='int')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c27f7-9248-44ac-a8e7-73057b27f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sklearn:\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot_encoder = LabelBinarizer()\n",
    "oh = one_hot_encoder.fit_transform(X['bird']) # One-hot encode the 'bird' variable\n",
    "print(oh)\n",
    "names = one_hot_encoder.classes_ # Get names of the classes\n",
    "print(names)\n",
    "\n",
    "zdf = pd.DataFrame( data = oh, columns = names) # Create a new df for the encoded variables\n",
    "zdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06482793-c9f1-42ce-ae09-20e9e5ab56f0",
   "metadata": {},
   "source": [
    "## The Dummy Variable Trap\n",
    "- When running linear regression with dummy variables, you have two choices:\n",
    "    - For each dummy variable, use the `pd.get_dummies(..., drop_constant=True)`, and keep the intercept in the regression\n",
    "    - Drop the intercept of the regression with the option `LinearRegression(fit_intercept=False).fit(X, y)`, but keep all the dummies\n",
    "- The reason this occurs is that if you have an intercept and all of the dummies, you can replicate one of your regressors from a combination of other ones. This is called **perfect multicolinearity**, and some of your coefficents won't be defined\n",
    "- Most stats packages hide this in the background of the regression, but you have to handle it with pandas/sklearn more explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295d17d-7155-405c-8b2e-3ecd955754f0",
   "metadata": {},
   "source": [
    "## Quickly Concatenating Dataframes\n",
    "- OK, you made these nice shiny features, how do you get them back into a single dataframe?\n",
    "- The `df = pd.concat([df1,df2,...,dfk],axis=1)` makes a new dataframe out of the columns of the original dataframes `df1`, `df2`, ..., `dfk`\n",
    "- So you can build all the transformed data frames you want, then collapse them all back into one dataframe for processing\n",
    "- Since you are going to do this, you might want to think carefully about how you're building the dataframe chunks, and use `.iloc` or `.loc` to be selective about what going into each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25a9a3-bd13-49ad-8afd-b258ab6077d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial features:\n",
    "expander = PolynomialFeatures(degree=2,include_bias=False) # Create the expander\n",
    "Z = expander.fit_transform(df.iloc[:,1:3]) # Pass the df into the expander to get powers/interactions of x and y\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "X1 = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "\n",
    "## Dummy variables:\n",
    "X2 = pd.get_dummies(df['bird'], dtype=float) # Create dummies\n",
    "\n",
    "## Concatenate:\n",
    "X = pd.concat([X1,X2],axis=1) # Concatenate engineered features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55261edc-4eee-4f89-a682-f72eb995046b",
   "metadata": {},
   "source": [
    "## Quickly Handling Outliers\n",
    "- We have the sense that outliers are bad, particularly for global smoothers like linear regression\n",
    "- We want to (1) detect outliers, (2) create an outlier dummy, and (3) **windsorize** them: Replace values outside the whiskers with the min/max values of the whiskers (when you remove outliers altogether, it is called **trimming**)\n",
    "- This is, again, a quick way of processing outliers without losing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f3f40-8071-4b10-a261-2fbdd2934814",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate random data with outliers\n",
    "np.random.seed(1000)\n",
    "N = 200\n",
    "switch = np.random.uniform(0,1,N)\n",
    "x = (switch<.05)*np.random.normal(20,10,N)+(switch >= .05)*(switch<.95)*np.random.normal(50,10,N)+(switch>=.95)*np.random.normal(100,5,N)\n",
    "sns.boxplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65028cd7-7ceb-455a-994c-d3e1eaf32fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Windsorize a variable:\n",
    "def winsorize(x):\n",
    "    # Convert from pd to np, if necessary:\n",
    "    if type(x) == 'pandas.core.series.Series':\n",
    "        x = x.to_numpy()\n",
    "    # Compute IQR and 25, 75 quantiles:\n",
    "    pct25, pct75 = np.percentile(x,[25,75])\n",
    "    iqr = pct75 - pct25\n",
    "    # Compute whiskers:\n",
    "    lower_whisker = pct25 - iqr*1.5\n",
    "    upper_whisker = pct75 + iqr*1.5\n",
    "    # Windorize x:\n",
    "    x_winsor = np.copy(x)\n",
    "    x_winsor[ x < lower_whisker ] = lower_whisker\n",
    "    x_winsor[ x > upper_whisker ] = upper_whisker\n",
    "    return(x_winsor)\n",
    "\n",
    "sns.boxplot( winsorize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b31044-c798-41cb-9b33-9e066c9b2723",
   "metadata": {},
   "source": [
    "## Cars, Again\n",
    "- Let's build some simple models to predict car prices that aren't constrained by our earlier concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3fd0d-b470-46b6-a1ba-073f5ebc6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0 # Let's start over, again\n",
    "df.head() # Glance at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdd1c6-3468-44ec-9004-191160f8a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple regression on brand dummies:\n",
    "\n",
    "# Target Variable\n",
    "y = df['price']\n",
    "\n",
    "# Brand Dummy:\n",
    "X_d = pd.get_dummies(df['brand'],dtype='int') # Create brand dummies\n",
    "\n",
    "# Regresion:\n",
    "from sklearn.linear_model import LinearRegression # Import linear regression model\n",
    "reg = LinearRegression(fit_intercept=False).fit(X_d, y) # Fit the linear model\n",
    "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_}) # Regression coefficients\n",
    "print('R-squared: ', reg.score(X_d, y)) # R squared measure\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032803d-0f7b-4b4f-9190-d337b23376ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More complex regression on brand and numeric values:\n",
    "\n",
    "# Target Variable\n",
    "y = df['price_ihs']\n",
    "\n",
    "# Initial boxplot:\n",
    "df.loc[:,['mileage_ihs','price_ihs','age']].plot.box()\n",
    "\n",
    "# Winsorize using .apply:\n",
    "df.loc[:, ['mileage_ihs','price_ihs','age'] ] = df.loc[:,['mileage_ihs','price_ihs','age'] ].apply(winsorize)\n",
    "\n",
    "# Final boxplot:\n",
    "df.loc[:,['mileage_ihs','price_ihs','age']].plot.box()\n",
    "\n",
    "# Expand numeric variables:\n",
    "expander = PolynomialFeatures(degree=1,include_bias=False) # Create the expander\n",
    "X_num = df.loc[:,['mileage_ihs']]\n",
    "Z = expander.fit_transform(X_num) # Pass the df into the expander to get powers/interactions of x and y\n",
    "names = expander.get_feature_names_out() # Get the names of these variables\n",
    "X_num = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
    "\n",
    "# Concatenate\n",
    "X = pd.concat( [X_num, X_d],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d9924-e815-49d8-a762-436795fbc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Import linear regression model\n",
    "reg = LinearRegression(fit_intercept=False).fit(X, y) # Fit the linear model\n",
    "results = pd.DataFrame({'variable':reg.feature_names_in_, 'coefficient': reg.coef_}) # Regression coefficients\n",
    "print('R-squared: ', reg.score(X, y)) # R squared measure\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48793d-be27-4cdc-9e87-fe6389f9ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['price_ihs'] -reg.predict(X) ) # Residual plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f462b-3081-4fcb-a7ea-bfd0acb5e5f1",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "- Datasets often have dozens or thousands of variables, and we can create many more through transforming and interacting variables. Which variables go into a linear regression?\n",
    "- In linear regression, our model is **underfit** if we use too few variables which cannot capture the complex relationships between the features and target variable, while our model is **overfit** if we are using too many variables which are exploiting too many unique features of the training data\n",
    "- In statistics, it is an assumption that the analyst roughly knows the \"true\" data generating process, and properties of an estimator are derived under that assumption - statistics has few useful answers for how to pick the form of the model itself (e.g. Akaike Information Criterion, Bayesian Information Criterion, Mallows' $C_p$)\n",
    "- In machine learning, we will learn some tools and techniques for model selection in a data-driven way later on in the course based on cross validation (e.g. LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7b127-d5a7-43e7-a5a9-ff4b8649aacc",
   "metadata": {},
   "source": [
    "## The Classical Assumptions for OLS [stats]\n",
    "- I often see people write the following: \"The neccessary assumptions for linear regression are (i) linearity, (ii) homoskedasticity, (iii) conditional independence of errors, (iv) normality of errors\"\n",
    "- This generates confusion --- These are the assumptions for *OLS to be the best, linear, unbiased estimator (BLUE) of a hypothetical \"true\" $b_0$ in a finite sample, and the $z$-test to be correctly specified*\n",
    "- We are not trying to estimate $b_0$, we are trying to do something else: find the *best linear predictor* of $y$ using the variables $X$. The above conditions are not necessary for that.\n",
    "- You don't need permission to run OLS and make predictions, you need permission to run OLS and interpret the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda309d-2053-4a2a-97a4-e78d08192fae",
   "metadata": {},
   "source": [
    "## General Linear Models\n",
    "- You can spend the rest of your life studying linear models and their generalizations\n",
    "- In many situations, there are additional restrictions on the environment that cannot always be satisfied by a linear model\n",
    "    - The outcome/target variable might be a binary 0/1 outcome, so we are predicting the probability of something occuring. OLS might predict values less than zero or greater than one. Popular solutions are **Logit regression** or **Probit regression**\n",
    "    - The outcome/target variable might be a non-negative integer, meaning that it is **count data**, like the number of earthquakes or cases of an illness. OLS won't predict counts. Popular solutions are **Poisson regression** or **negative binomial regression**.\n",
    "- In these cases, we can estimate alternative models that impose restrictions on the outcome of the linear model, typically through maximum likelihood estimation or generalized linear modeling\n",
    "- These aren't typically much more work to estimate, but we don't really have the time to cover all the nuances (and people typically estimate them only if the OLS model really breaks down in terms of predictive performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
