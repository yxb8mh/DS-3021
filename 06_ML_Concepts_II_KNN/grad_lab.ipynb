{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683ac305",
   "metadata": {},
   "source": [
    "# Graduation Lab (Week 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2d74c",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "Let's build a kNN model using the college completion data.\n",
    "The data is messy and you have a degrees of freedom problem, as in, we have too many features.\n",
    "\n",
    "You've done most of the hard work already, so you should be ready to move forward with building your model.\n",
    "\n",
    "1. Use the question/target variable you submitted and\n",
    "build a model to answer the question you created for this dataset (make sure it is a classification problem, convert if necessary).\n",
    "2. Build a kNN model to predict your target variable using 3 nearest neighbors. Make sure it is a classification problem, meaning\n",
    "if needed changed the target variable.\n",
    "3. Create a dataframe that includes the test target values, test predicted values,\n",
    "and test probabilities of the positive class.\n",
    "4. No code question: If you adjusted the k hyperparameter what do you think would\n",
    "happen to the threshold function? Would the confusion look the same at the same threshold\n",
    "levels or not? Why or why not?\n",
    "5. Evaluate the results using the confusion matrix. Then \"walk\" through your question, summarize what\n",
    "concerns or positive elements do you have about the model as it relates to your question?\n",
    "6. Create two functions: One that cleans the data & splits into training|test and one that\n",
    "allows you to train and test the model with different k and threshold values, then use them to\n",
    "optimize your model (test your model with several k and threshold combinations). Try not to use variable names\n",
    "in the functions, but if you need to that's fine. (If you can't get the k function and threshold function to work in one\n",
    "function just run them separately.)\n",
    "7. How well does the model perform? Did the interaction of the adjusted thresholds and k values help the model? Why or why not?\n",
    "8. Choose another variable as the target in the dataset and create another kNN model using the two functions you created in\n",
    "step 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76dc36",
   "metadata": {},
   "source": [
    "### 1. Use the question/target variable you submitted and build a model to answer the question you created for this dataset (make sure it is a classification problem, convert if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66a08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  unitid                            chronname        city    state  \\\n",
      "0      0  100654               Alabama A&M University      Normal  Alabama   \n",
      "1      1  100663  University of Alabama at Birmingham  Birmingham  Alabama   \n",
      "2      2  100690                   Amridge University  Montgomery  Alabama   \n",
      "3      3  100706  University of Alabama at Huntsville  Huntsville  Alabama   \n",
      "4      4  100724             Alabama State University  Montgomery  Alabama   \n",
      "\n",
      "    level                 control  \\\n",
      "0  4-year                  Public   \n",
      "1  4-year                  Public   \n",
      "2  4-year  Private not-for-profit   \n",
      "3  4-year                  Public   \n",
      "4  4-year                  Public   \n",
      "\n",
      "                                               basic hbcu flagship  ...  \\\n",
      "0  Masters Colleges and Universities--larger prog...    X      NaN  ...   \n",
      "1  Research Universities--very high research acti...  NaN      NaN  ...   \n",
      "2            Baccalaureate Colleges--Arts & Sciences  NaN      NaN  ...   \n",
      "3  Research Universities--very high research acti...  NaN      NaN  ...   \n",
      "4  Masters Colleges and Universities--larger prog...    X      NaN  ...   \n",
      "\n",
      "   vsa_grad_after6_transfer  vsa_grad_elsewhere_after6_transfer  \\\n",
      "0                      36.4                                 5.6   \n",
      "1                       NaN                                 NaN   \n",
      "2                       NaN                                 NaN   \n",
      "3                       0.0                                 0.0   \n",
      "4                       NaN                                 NaN   \n",
      "\n",
      "  vsa_enroll_after6_transfer  vsa_enroll_elsewhere_after6_transfer  \\\n",
      "0                       17.2                                  11.1   \n",
      "1                        NaN                                   NaN   \n",
      "2                        NaN                                   NaN   \n",
      "3                        0.0                                   0.0   \n",
      "4                        NaN                                   NaN   \n",
      "\n",
      "                                             similar  state_sector_ct  \\\n",
      "0  232937|100724|405997|113607|139533|144005|2285...               13   \n",
      "1  196060|180461|201885|145600|209542|236939|1268...               13   \n",
      "2  217925|441511|205124|247825|197647|221856|1353...               16   \n",
      "3  232186|133881|196103|196413|207388|171128|1900...               13   \n",
      "4  100654|232937|242617|243197|144005|241739|2354...               13   \n",
      "\n",
      "   carnegie_ct  counted_pct  nicknames  cohort_size  \n",
      "0          386      99.7|07        NaN        882.0  \n",
      "1          106      56.0|07        UAB       1376.0  \n",
      "2          252     100.0|07        NaN          3.0  \n",
      "3          106      43.1|07        UAH        759.0  \n",
      "4          386      88.0|07        ASU       1351.0  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "Index(['index', 'unitid', 'chronname', 'city', 'state', 'level', 'control',\n",
      "       'basic', 'hbcu', 'flagship', 'long_x', 'lat_y', 'site', 'student_count',\n",
      "       'awards_per_value', 'awards_per_state_value', 'awards_per_natl_value',\n",
      "       'exp_award_value', 'exp_award_state_value', 'exp_award_natl_value',\n",
      "       'exp_award_percentile', 'ft_pct', 'fte_value', 'fte_percentile',\n",
      "       'med_sat_value', 'med_sat_percentile', 'aid_value', 'aid_percentile',\n",
      "       'endow_value', 'endow_percentile', 'grad_100_value',\n",
      "       'grad_100_percentile', 'grad_150_value', 'grad_150_percentile',\n",
      "       'pell_value', 'pell_percentile', 'retain_value', 'retain_percentile',\n",
      "       'ft_fac_value', 'ft_fac_percentile', 'vsa_year',\n",
      "       'vsa_grad_after4_first', 'vsa_grad_elsewhere_after4_first',\n",
      "       'vsa_enroll_after4_first', 'vsa_enroll_elsewhere_after4_first',\n",
      "       'vsa_grad_after6_first', 'vsa_grad_elsewhere_after6_first',\n",
      "       'vsa_enroll_after6_first', 'vsa_enroll_elsewhere_after6_first',\n",
      "       'vsa_grad_after4_transfer', 'vsa_grad_elsewhere_after4_transfer',\n",
      "       'vsa_enroll_after4_transfer', 'vsa_enroll_elsewhere_after4_transfer',\n",
      "       'vsa_grad_after6_transfer', 'vsa_grad_elsewhere_after6_transfer',\n",
      "       'vsa_enroll_after6_transfer', 'vsa_enroll_elsewhere_after6_transfer',\n",
      "       'similar', 'state_sector_ct', 'carnegie_ct', 'counted_pct', 'nicknames',\n",
      "       'cohort_size'],\n",
      "      dtype='object')\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               high_aid   No. Observations:                 3797\n",
      "Model:                          Logit   Df Residuals:                     3796\n",
      "Method:                           MLE   Df Model:                            0\n",
      "Date:                Sat, 08 Mar 2025   Pseudo R-squ.:               1.289e-10\n",
      "Time:                        05:50:23   Log-Likelihood:                -2631.9\n",
      "converged:                       True   LL-Null:                       -2631.9\n",
      "Covariance Type:            nonrobust   LLR p-value:                       nan\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "inst_type     -0.0005      0.032     -0.016      0.987      -0.064       0.063\n",
      "==============================================================================\n",
      "[[1899    0]\n",
      " [1898    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      1.00      0.67      1899\n",
      "         1.0       0.00      0.00      0.00      1898\n",
      "\n",
      "    accuracy                           0.50      3797\n",
      "   macro avg       0.25      0.50      0.33      3797\n",
      "weighted avg       0.25      0.50      0.33      3797\n",
      "\n",
      "   inst_type  aid_value  high_aid  pred_prob  predicted\n",
      "0          1     7142.0         1   0.499868          0\n",
      "1          1     6088.0         1   0.499868          0\n",
      "2          1     2540.0         0   0.499868          0\n",
      "3          1     6647.0         1   0.499868          0\n",
      "4          1     7256.0         1   0.499868          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vscode/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/vscode/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/workspaces/DS-3021/data/cc_institution_details.csv')\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(data.columns)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#combine private for profit and not for profit together\n",
    "data['inst_type'] = data['control'].apply(lambda x: 'public' if x == 1 else 'private')\n",
    "#create two categories for aid_value, first find the median, if it's above the median then it's high aid, vice versa\n",
    "median_aid = data['aid_value'].median()\n",
    "data['high_aid'] = (data['aid_value'] > median_aid).astype(int)\n",
    "\n",
    "\n",
    "#make sure control and aid are numbers\n",
    "data['control'] = pd.to_numeric(data['control'], errors='coerce')\n",
    "data['aid_value'] = pd.to_numeric(data['aid_value'], errors='coerce')\n",
    "\n",
    "#categorizing them into binary\n",
    "data['inst_type'] = data['control'].apply(lambda x: 0 if x == 1 else 1).astype(int)\n",
    "median_aid = data['aid_value'].median()\n",
    "data['high_aid'] = (data['aid_value'] > median_aid).astype(int)\n",
    "\n",
    "#clean data \n",
    "data = data.dropna(subset=['inst_type', 'aid_value', 'high_aid'])\n",
    "\n",
    "#using institution type as the predictor to build the LRM here \n",
    "X = data[['inst_type']].astype(float)\n",
    "X = sm.add_constant(X) \n",
    "y = data['high_aid'].astype(float)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit(disp=0)\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "#predict proabailities\n",
    "data['pred_prob'] = result.predict(X)\n",
    "data['predicted'] = (data['pred_prob'] >= 0.5).astype(int)\n",
    "\n",
    "#use confusion matrix and classification report\n",
    "cm = confusion_matrix(y, data['predicted'])\n",
    "print(cm)\n",
    "print(classification_report(y, data['predicted']))\n",
    "\n",
    "#print first 5 rows to check the actual and predicted data\n",
    "print(data[['inst_type', 'aid_value', 'high_aid', 'pred_prob', 'predicted']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28feee4",
   "metadata": {},
   "source": [
    "### 2. Build a kNN model to predict your target variable using 3 nearest neighbors. Make sure it is a classification problem, meaning if needed changed the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2bcb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  Probability\n",
      "2171       1          1     0.666667\n",
      "1223       0          0     0.000000\n",
      "2229       1          0     0.000000\n",
      "2391       1          1     1.000000\n",
      "2575       0          0     0.000000\n",
      "705        1          1     1.000000\n",
      "1333       1          1     0.666667\n",
      "1417       1          1     1.000000\n",
      "1419       1          1     1.000000\n",
      "1273       0          1     0.666667\n",
      "[[154  67]\n",
      " [ 74 402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       221\n",
      "           1       0.86      0.84      0.85       476\n",
      "\n",
      "    accuracy                           0.80       697\n",
      "   macro avg       0.77      0.77      0.77       697\n",
      "weighted avg       0.80      0.80      0.80       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['inst_type', 'student_count', 'endow_value', 'fte_value'] #use a couple other features to predict the aid value\n",
    "data_clean = data.dropna(subset=features + ['high_aid'])\n",
    "\n",
    "X = data_clean[features]\n",
    "y = data_clean['high_aid']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#make knn classifier \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Probability': y_prob})\n",
    "print(results.head(10))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f96b2",
   "metadata": {},
   "source": [
    "### 3. Create a dataframe that includes the test target values, test predicted values, and test probabilities of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4338bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted  Probability_Positive\n",
      "2171       1          1              0.666667\n",
      "1223       0          0              0.000000\n",
      "2229       1          0              0.000000\n",
      "2391       1          1              1.000000\n",
      "2575       0          0              0.000000\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "#get predicted probabilities for positive class\n",
    "y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "#make df\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Probability_Positive': y_prob\n",
    "})\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c87c1e",
   "metadata": {},
   "source": [
    "### 4. No code question: If you adjusted the k hyperparameter what do you think would happen to the threshold function? Would the confusion look the same at the same threshold levels or not? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I think changing the k hyperparameter would change the distribution of the predicted probabilities. if it's a lower k, the model will be more sensitive to noise; a higher k will smooth out the local changes (because you are taking into consideration of more neighbors), hence more stable probabilities. if you keep the same threshold, the confusion matrix will look different because the predicted probabilities are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the results using the confusion matrix. Then \"walk\" through your question, summarize what concerns or positive elements do you have about the model as it relates to your question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb4f54",
   "metadata": {},
   "source": [
    "##### from the confusion matrix we can tell that the model finds 154 universities as low aid schools and 402 as high aid schools. but it also misidentified 67 schools that are actually low aid as high aid and vice versa for 74 schools. the overall performance is pretty good. my questions examines whether private (inclding for profit and not for profit) universities or public schools tend to offer more financial aid. the model predicts that public schools are more likely to offer high aid, which is consistent with my expectation. yet, the 74 false negatives are a concern, suggesting that we may need more variables to accurately identify low aid schools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae9adf",
   "metadata": {},
   "source": [
    "### 6. Create two functions: One that cleans the data & splits into training|test and one that allows you to train and test the model with different k and threshold values, then use them to optimize your model (test your model with several k and threshold combinations). Try not to use variable names in the functions, but if you need to that's fine. (If you can't get the k function and threshold function to work in one function just run them separately.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4c653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function data_clean_split at 0xffff302d71a0>\n"
     ]
    }
   ],
   "source": [
    "#function 1\n",
    "\n",
    "def data_clean_split(data, control_col, aid_col, features, test_size=0.3, random_state=42):\n",
    "    #same as above\n",
    "    data[control_col] = pd.to_numeric(data[control_col], errors='coerce')\n",
    "    data[aid_col] = pd.to_numeric(data[aid_col], errors='coerce')\n",
    "    data['inst_type'] = data[control_col].apply(lambda x: 0 if x == 1 else 1)\n",
    "    median_val = data[aid_col].median()\n",
    "    data['high_aid'] = (data[aid_col] > median_val).astype(int)\n",
    "    required_cols = features + ['inst_type', 'high_aid']\n",
    "    cleaned_data = data.dropna(subset=required_cols)\n",
    "    \n",
    "    #create the features and target\n",
    "    X = cleaned_data[features + ['inst_type']] \n",
    "    y = cleaned_data['high_aid']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state)\n",
    "    #scale X for knn \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "print(data_clean_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1265e42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 1, 'threshold': 0.3, 'confusion_matrix': array([[152,  69],\n",
      "       [ 93, 383]]), 'accuracy': 0.7675753228120517}\n",
      "{'k': 1, 'threshold': 0.5, 'confusion_matrix': array([[152,  69],\n",
      "       [ 93, 383]]), 'accuracy': 0.7675753228120517}\n",
      "{'k': 1, 'threshold': 0.7, 'confusion_matrix': array([[152,  69],\n",
      "       [ 93, 383]]), 'accuracy': 0.7675753228120517}\n",
      "{'k': 3, 'threshold': 0.3, 'confusion_matrix': array([[ 73, 148],\n",
      "       [ 22, 454]]), 'accuracy': 0.7560975609756098}\n",
      "{'k': 3, 'threshold': 0.5, 'confusion_matrix': array([[154,  67],\n",
      "       [ 74, 402]]), 'accuracy': 0.7977044476327116}\n",
      "{'k': 3, 'threshold': 0.7, 'confusion_matrix': array([[193,  28],\n",
      "       [186, 290]]), 'accuracy': 0.6929698708751794}\n",
      "{'k': 5, 'threshold': 0.3, 'confusion_matrix': array([[106, 115],\n",
      "       [ 29, 447]]), 'accuracy': 0.793400286944046}\n",
      "{'k': 5, 'threshold': 0.5, 'confusion_matrix': array([[144,  77],\n",
      "       [ 69, 407]]), 'accuracy': 0.7905308464849354}\n",
      "{'k': 5, 'threshold': 0.7, 'confusion_matrix': array([[180,  41],\n",
      "       [121, 355]]), 'accuracy': 0.7675753228120517}\n",
      "{'k': 7, 'threshold': 0.3, 'confusion_matrix': array([[118, 103],\n",
      "       [ 30, 446]]), 'accuracy': 0.8091822094691535}\n",
      "{'k': 7, 'threshold': 0.5, 'confusion_matrix': array([[148,  73],\n",
      "       [ 62, 414]]), 'accuracy': 0.806312769010043}\n",
      "{'k': 7, 'threshold': 0.7, 'confusion_matrix': array([[169,  52],\n",
      "       [ 93, 383]]), 'accuracy': 0.7919655667144907}\n",
      "{'k': 9, 'threshold': 0.3, 'confusion_matrix': array([[ 96, 125],\n",
      "       [ 21, 455]]), 'accuracy': 0.7905308464849354}\n",
      "{'k': 9, 'threshold': 0.5, 'confusion_matrix': array([[142,  79],\n",
      "       [ 55, 421]]), 'accuracy': 0.8077474892395983}\n",
      "{'k': 9, 'threshold': 0.7, 'confusion_matrix': array([[188,  33],\n",
      "       [133, 343]]), 'accuracy': 0.7618364418938307}\n"
     ]
    }
   ],
   "source": [
    "#function 2\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def knn_train_test(X_train_scaled, X_test_scaled, y_train, y_test, k_values, thresholds):\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        #predicted probabilities for the positive class \n",
    "        y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "            cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "            acc = accuracy_score(y_test, y_pred_thresh)\n",
    "\n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'threshold': thresh,\n",
    "                'confusion_matrix': cm,\n",
    "                'accuracy': acc\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "#define columns\n",
    "control_col = 'control'\n",
    "aid_col = 'aid_value'\n",
    "features = ['student_count', 'endow_value', 'fte_value']  # Example features\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = data_clean_split(data, control_col, aid_col, features)\n",
    "\n",
    "#try with multiple k and threshold values\n",
    "k_list = [1, 3, 5, 7, 9]\n",
    "threshold_list = [0.3, 0.5, 0.7]\n",
    "results = knn_train_test(X_train_scaled, X_test_scaled, y_train, y_test, k_list, threshold_list)\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8095a",
   "metadata": {},
   "source": [
    "### 7. How well does the model perform? Did the interaction of the adjusted thresholds and k values help the model? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8869c",
   "metadata": {},
   "source": [
    "##### From the output, it’s clear that the model’s performance (measured by accuracy and the confusion matrix) changes depending on the values of k and the threshold, meaning that tweaking these parameters can actually impact how well the classifier differentiates between classes. A smaller k (like 1 or 3) makes the model more sensitive to local data points, which can lead to higher variance in predictions. a larger k (like 7 or 9) smooths things out by averaging over more neighbors. Meanwhile, adjusting the threshold affects the trade-off between getting more true positives (with a lower threshold) and avoiding false positives (with a higher threshold). If certain k and threshold combinations consistently improve accuracy, that’s a good sign that tuning these parameters is worth it. But if the improvements are inconsistent, it might mean that adding more features or trying a different modeling approach would be a better way to boost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f8e5e",
   "metadata": {},
   "source": [
    "### 8. Choose another variable as the target in the dataset and create another kNN model using the two functions you created in step 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2c0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 1, 'threshold': 0.3, 'confusion_matrix': array([[339, 131],\n",
      "       [122, 105]]), 'accuracy': 0.6370157819225251, 'true_neg': np.int64(339), 'false_pos': np.int64(131), 'false_neg': np.int64(122), 'true_pos': np.int64(105)}\n",
      "{'k': 1, 'threshold': 0.5, 'confusion_matrix': array([[339, 131],\n",
      "       [122, 105]]), 'accuracy': 0.6370157819225251, 'true_neg': np.int64(339), 'false_pos': np.int64(131), 'false_neg': np.int64(122), 'true_pos': np.int64(105)}\n",
      "{'k': 1, 'threshold': 0.7, 'confusion_matrix': array([[339, 131],\n",
      "       [122, 105]]), 'accuracy': 0.6370157819225251, 'true_neg': np.int64(339), 'false_pos': np.int64(131), 'false_neg': np.int64(122), 'true_pos': np.int64(105)}\n",
      "{'k': 3, 'threshold': 0.3, 'confusion_matrix': array([[194, 276],\n",
      "       [ 49, 178]]), 'accuracy': 0.533715925394548, 'true_neg': np.int64(194), 'false_pos': np.int64(276), 'false_neg': np.int64(49), 'true_pos': np.int64(178)}\n",
      "{'k': 3, 'threshold': 0.5, 'confusion_matrix': array([[360, 110],\n",
      "       [125, 102]]), 'accuracy': 0.6628407460545194, 'true_neg': np.int64(360), 'false_pos': np.int64(110), 'false_neg': np.int64(125), 'true_pos': np.int64(102)}\n",
      "{'k': 3, 'threshold': 0.7, 'confusion_matrix': array([[436,  34],\n",
      "       [198,  29]]), 'accuracy': 0.667144906743185, 'true_neg': np.int64(436), 'false_pos': np.int64(34), 'false_neg': np.int64(198), 'true_pos': np.int64(29)}\n",
      "{'k': 5, 'threshold': 0.3, 'confusion_matrix': array([[249, 221],\n",
      "       [ 64, 163]]), 'accuracy': 0.5911047345767575, 'true_neg': np.int64(249), 'false_pos': np.int64(221), 'false_neg': np.int64(64), 'true_pos': np.int64(163)}\n",
      "{'k': 5, 'threshold': 0.5, 'confusion_matrix': array([[378,  92],\n",
      "       [129,  98]]), 'accuracy': 0.6829268292682927, 'true_neg': np.int64(378), 'false_pos': np.int64(92), 'false_neg': np.int64(129), 'true_pos': np.int64(98)}\n",
      "{'k': 5, 'threshold': 0.7, 'confusion_matrix': array([[434,  36],\n",
      "       [182,  45]]), 'accuracy': 0.6872309899569584, 'true_neg': np.int64(434), 'false_pos': np.int64(36), 'false_neg': np.int64(182), 'true_pos': np.int64(45)}\n",
      "{'k': 7, 'threshold': 0.3, 'confusion_matrix': array([[293, 177],\n",
      "       [ 70, 157]]), 'accuracy': 0.6456241032998565, 'true_neg': np.int64(293), 'false_pos': np.int64(177), 'false_neg': np.int64(70), 'true_pos': np.int64(157)}\n",
      "{'k': 7, 'threshold': 0.5, 'confusion_matrix': array([[377,  93],\n",
      "       [130,  97]]), 'accuracy': 0.6800573888091822, 'true_neg': np.int64(377), 'false_pos': np.int64(93), 'false_neg': np.int64(130), 'true_pos': np.int64(97)}\n",
      "{'k': 7, 'threshold': 0.7, 'confusion_matrix': array([[430,  40],\n",
      "       [176,  51]]), 'accuracy': 0.6901004304160688, 'true_neg': np.int64(430), 'false_pos': np.int64(40), 'false_neg': np.int64(176), 'true_pos': np.int64(51)}\n"
     ]
    }
   ],
   "source": [
    "#using pell\n",
    "def data_clean_split_pell(data, control_col, pell_col, features, test_size=0.3, random_state=42):\n",
    "    data[control_col] = pd.to_numeric(data[control_col], errors='coerce')\n",
    "    data[pell_col] = pd.to_numeric(data[pell_col], errors='coerce')\n",
    "    data['inst_type'] = data[control_col].apply(lambda x: 0 if x == 1 else 1)\n",
    "    median_val = data[pell_col].median()\n",
    "    data['high_pell'] = (data[pell_col] > median_val).astype(int)\n",
    "    required_cols = features + ['inst_type', 'high_pell']\n",
    "    cleaned_data = data.dropna(subset=required_cols)\n",
    "    X = cleaned_data[features + ['inst_type']]\n",
    "    y = cleaned_data['high_pell']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "#knn training/testing function (same as above)\n",
    "def knn_train_test(X_train_scaled, X_test_scaled, y_train, y_test, k_values, thresholds):\n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "\n",
    "            cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "            acc = accuracy_score(y_test, y_pred_thresh)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'threshold': thresh,\n",
    "                'confusion_matrix': cm,\n",
    "                'accuracy': acc,\n",
    "                'true_neg': tn,\n",
    "                'false_pos': fp,\n",
    "                'false_neg': fn,\n",
    "                'true_pos': tp\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "control_col = 'control'\n",
    "pell_col = 'pell_value'\n",
    "features = ['student_count', 'endow_value', 'fte_value']\n",
    "X_train_scaled_pell, X_test_scaled_pell, y_train_pell, y_test_pell = data_clean_split_pell(data, control_col, pell_col, features)\n",
    "\n",
    "#here we define the lists of k and threshold values used to test\n",
    "k_list = [1, 3, 5, 7]\n",
    "threshold_list = [0.3, 0.5, 0.7]\n",
    "pell_results = knn_train_test(X_train_scaled_pell, X_test_scaled_pell, y_train_pell, y_test_pell, k_list, threshold_list)\n",
    "for res in pell_results:\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
