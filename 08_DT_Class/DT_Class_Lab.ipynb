{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! You just graduated UVA's BSDS program and got a job working at a movie studio in Hollywood.\n",
    "\n",
    "Your boss is the head of the studio and wants to know if they can gain a competitive advantage by predicting new movies that might get high imdb scores (movie rating).\n",
    "\n",
    "You would like to be able to explain the model to mere mortals but need a fairly robust and flexible approach so you've chosen to use decision trees to get started.\n",
    "\n",
    "In doing so, similar to great data scientists of the past you remembered the excellent education provided to you at UVA in a undergrad data science course and have outline 20ish steps that will need to be undertaken to complete this task. As always, you will need to make sure to #comment your work heavily.\n",
    "\n",
    "Footnotes:\n",
    "\n",
    "You can add or combine steps if needed\n",
    "Also, remember to try several methods during evaluation and always be mindful of how the model will be used in practice.\n",
    "Make sure all your variables are the correct type (factor, character,numeric, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.preprocessing as LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load the data\n",
    "#Sometimes need to set the working directory back out of a folder that we create a file in\n",
    "\n",
    "#import os\n",
    "#os.listdir()\n",
    "#print(os.getcwd())\n",
    "#os.chdir('c:\\\\Users\\\\Brian Wright\\\\Documents\\\\3001Python\\\\DS-3001')\n",
    "\n",
    "movie_metadata=pd.read_csv(\"/workspaces/DS-3021/data/movie_metadata.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Ensure all the variables are classified correctly including the target variable and collapse factor variables as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color                        category\n",
      "director_name                category\n",
      "num_critic_for_reviews        float64\n",
      "duration                      float64\n",
      "director_facebook_likes       float64\n",
      "actor_3_facebook_likes        float64\n",
      "actor_2_name                 category\n",
      "actor_1_facebook_likes        float64\n",
      "gross                         float64\n",
      "genres                       category\n",
      "actor_1_name                 category\n",
      "movie_title                    object\n",
      "num_voted_users                 int64\n",
      "cast_total_facebook_likes       int64\n",
      "actor_3_name                 category\n",
      "facenumber_in_poster          float64\n",
      "plot_keywords                category\n",
      "movie_imdb_link                object\n",
      "num_user_for_reviews          float64\n",
      "language                     category\n",
      "country                      category\n",
      "content_rating               category\n",
      "budget                        float64\n",
      "title_year                    float64\n",
      "actor_2_facebook_likes        float64\n",
      "imdb_score                    float64\n",
      "aspect_ratio                  float64\n",
      "movie_facebook_likes            int64\n",
      "high_rating                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Ensure Variables are Classified Correctly\n",
    "# -------------------------------\n",
    "\n",
    "# Create a binary target variable 'high_rating'\n",
    "# For example, movies with an IMDb score of 7.0 or higher are marked as 1 (high rating), else 0.\n",
    "movie_metadata['high_rating'] = (movie_metadata['imdb_score'] >= 7.0).astype(int) #define high rating \n",
    "\n",
    "# -------------------------------\n",
    "# Convert numeric columns to appropriate numeric types\n",
    "# -------------------------------\n",
    "# List numeric columns that should be in a numeric format.\n",
    "numeric_columns = [\n",
    "    'num_critic_for_reviews', 'duration', 'director_facebook_likes',\n",
    "    'actor_3_facebook_likes', 'actor_1_facebook_likes', 'gross',\n",
    "    'num_voted_users', 'cast_total_facebook_likes', 'facenumber_in_poster',\n",
    "    'num_user_for_reviews', 'budget', 'title_year', 'actor_2_facebook_likes',\n",
    "    'imdb_score', 'aspect_ratio', 'movie_facebook_likes'\n",
    "]\n",
    "\n",
    "# Convert each column to a numeric data type, coercing errors to NaN.\n",
    "for col in numeric_columns:\n",
    "    movie_metadata[col] = pd.to_numeric(movie_metadata[col], errors='coerce')\n",
    "\n",
    "# -------------------------------\n",
    "# Convert specific columns to categorical (factor) type\n",
    "# -------------------------------\n",
    "# Define columns that represent categorical data.\n",
    "factor_columns = [\n",
    "    'color', 'director_name', 'actor_2_name', 'actor_1_name', 'genres',\n",
    "    'actor_3_name', 'plot_keywords', 'language', 'country', 'content_rating'\n",
    "]\n",
    "\n",
    "# Convert each of these columns to the 'category' dtype.\n",
    "for col in factor_columns:\n",
    "    movie_metadata[col] = movie_metadata[col].astype('category')\n",
    "\n",
    "# -------------------------------\n",
    "# Collapse infrequent factor levels\n",
    "# -------------------------------\n",
    "def collapse_categories(series, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Replace categories in a pandas Series that occur in less than 'threshold' fraction\n",
    "    of the total entries with 'Other'.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The categorical series to process.\n",
    "        threshold (float): The minimum fraction required to keep a category.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: The modified series with infrequent categories replaced.\n",
    "    \"\"\"\n",
    "    # Calculate relative frequency of each category.\n",
    "    freq = series.value_counts(normalize=True)\n",
    "    # Identify categories with a frequency less than the threshold.\n",
    "    categories_to_collapse = freq[freq < threshold].index\n",
    "    # Replace infrequent categories with 'Other'\n",
    "    return series.apply(lambda x: 'Other' if x in categories_to_collapse else x)\n",
    "\n",
    "# Apply the collapsing function to each categorical column.\n",
    "for col in factor_columns:\n",
    "    movie_metadata[col] = collapse_categories(movie_metadata[col], threshold=0.05)\n",
    "    # Reaffirm the categorical type after collapsing levels.\n",
    "    movie_metadata[col] = movie_metadata[col].astype('category')\n",
    "\n",
    "# -------------------------------\n",
    "# (Optional) Check data types to verify correct conversion\n",
    "# -------------------------------\n",
    "print(movie_metadata.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Check for missing variables and correct as needed. Once you've completed the cleaning again create a function that will do this for you in the future. In the submission, include only the function and the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_896/2537868089.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_896/2537868089.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Missing', inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_896/2537868089.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Missing', inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_896/2537868089.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Missing', inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_896/2537868089.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Missing', inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(df[col]):\n",
      "/tmp/ipykernel_896/2537868089.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Missing', inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/tmp/ipykernel_896/2537868089.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_missing_values(df):\n",
    "    \"\"\"\n",
    "    Cleans missing values in the DataFrame by:\n",
    "      - For numeric columns: replacing missing values with the median.\n",
    "      - For categorical columns: adding 'Missing' as a category if needed, then replacing missing values.\n",
    "      - For other non-numeric columns: replacing missing values with the string 'Missing'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be cleaned.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # For numeric columns, impute missing values with the median.\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            # For categorical columns, add 'Missing' to categories if not present, then fill.\n",
    "            elif pd.api.types.is_categorical_dtype(df[col]):\n",
    "                if 'Missing' not in df[col].cat.categories:\n",
    "                    df[col] = df[col].cat.add_categories(['Missing'])\n",
    "                df[col].fillna('Missing', inplace=True)\n",
    "            # For non-numeric and non-categorical columns, fill with 'Missing'.\n",
    "            else:\n",
    "                df[col].fillna('Missing', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Function call to clean missing values in the movie_metadata DataFrame.\n",
    "movie_metadata = clean_missing_values(movie_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m     y = LabelEncoder().fit_transform(y)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m X, y = \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#this is the function that is going to be used to preprocess the data that is going to be used for this code\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m#ran into a lot of trouble with the cateorical variables so for content_rating for example i minimized the options to R, PG-13, PG, and Not Rated\u001b[39;00m\n\u001b[32m     59\u001b[39m   \u001b[38;5;66;03m#i then labeled encoded them so that they had numerical values and could then be used in the model\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#i then dropped the columns that were not going to be used in the model and then dropped any rows that had missing values\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m#i then set the X and y variables to the data that was going to be used in the model\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m#i then returned the X and y variables\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpreprocess_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     10\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m\"\u001b[39m].replace({\n\u001b[32m     11\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mUnrated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNot Rated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mNot Rated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNot Rated\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     13\u001b[39m ).apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPG-13\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNot Rated\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mOther\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.fit_transform(df[\u001b[33m'\u001b[39m\u001b[33mcontent_rating\u001b[39m\u001b[33m'\u001b[39m]) \n\u001b[32m     18\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x == \u001b[33m\"\u001b[39m\u001b[33mColor\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m     19\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df.dropna(subset=[\"imdb_score\"])\n",
    "    df[\"rating_category\"] = np.where(df[\"imdb_score\"] > 7.2, \"good\", \"bad\")\n",
    "    df[\"rating_category\"] = df[\"imdb_score\"].apply(lambda x: \"good\" if x > 7.2 else \"bad\")\n",
    "\n",
    "    drop_columns = [\"actor_1_name\", \"actor_3_name\", \"movie_title\", \"movie_imdb_link\", \"director_name\", \"director_facebook_likes\", \"actor_1_facebook_likes\", \"actor_2_facebook_likes\", \"actor_3_facebook_likes\", \"actor_2_name\", \"aspect_ratio\", \"facenumber_in_poster\", \"num_critic_for_reviews\", \"num_user_for_reviews\", \"movie_facebook_likes\", \"cast_total_facebook_likes\",]\n",
    "    df = df.drop(columns=drop_columns, errors=\"ignore\")\n",
    "    \n",
    "\n",
    "    df[\"content_rating\"] = df[\"content_rating\"].replace({\n",
    "    \"Unrated\": \"Not Rated\",\n",
    "    \"Not Rated\": \"Not Rated\"}\n",
    "    ).apply(lambda x: x if x in [\"R\", \"PG-13\", \"PG\", \"Not Rated\"] else \"Other\")\n",
    "    df[\"content_rating\"] = df[\"content_rating\"].astype('category')\n",
    "    df['content_rating'] = LabelEncoder().fit_transform(df['content_rating']) \n",
    "\n",
    "\n",
    "    df[\"color\"] = df[\"color\"].apply(lambda x: 1 if x == \"Color\" else 0)\n",
    "    df[\"color\"] = df[\"color\"].astype('category')\n",
    "    df['color'] = LabelEncoder().fit_transform(df['color']) \n",
    "\n",
    "\n",
    "    df[\"plot_keywords\"] = df[\"plot_keywords\"].apply(\n",
    "        lambda x: x.split(\"|\")[0] if isinstance(x, str) and \"|\" in x else x\n",
    "        )\n",
    "    df[\"plot_keywords\"] = df[\"plot_keywords\"].astype('category')\n",
    "    df['plot_keywords'] = LabelEncoder().fit_transform(df['plot_keywords']) \n",
    "\n",
    "\n",
    "    df[\"genres\"] = df[\"genres\"].apply(\n",
    "          lambda x: x.split(\"|\")[0] if isinstance(x, str) and \"|\" in x else x\n",
    "        )\n",
    "    df[\"genres\"] = df[\"genres\"].astype('category')\n",
    "    df['genres'] = LabelEncoder().fit_transform(df['genres']) \n",
    "\n",
    "    \n",
    "    df['country'] = df['country'].apply(lambda x: x if x in [\"USA\", \"UK\", \"France\", \"Canada\", \"Germany\"] else \"Other\")\n",
    "    df[\"country\"] = df[\"country\"].astype('category')\n",
    "    df['country'] = LabelEncoder().fit_transform(df['country']) \n",
    "\n",
    "\n",
    "    df['language'] = df['language'].apply(lambda x: x if x in [\"English\", \"French\", \"Spanish\"] else \"Other\")\n",
    "    df[\"language\"] = df[\"language\"].astype('category')\n",
    "    df['language'] = LabelEncoder().fit_transform(df['language']) \n",
    "\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    X = df.drop(columns=[\"rating_category\", \"imdb_score\"])\n",
    "    y = df[\"rating_category\"]\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_data(movie_metadata)\n",
    "\n",
    "#this is the function that is going to be used to preprocess the data that is going to be used for this code\n",
    "#ran into a lot of trouble with the cateorical variables so for content_rating for example i minimized the options to R, PG-13, PG, and Not Rated\n",
    "  #i then labeled encoded them so that they had numerical values and could then be used in the model\n",
    "#i then dropped the columns that were not going to be used in the model and then dropped any rows that had missing values\n",
    "#i then set the X and y variables to the data that was going to be used in the model\n",
    "#i then returned the X and y variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 Guess what, you don't need to scale the data, because DTs don't require this to be done, they make local greedy decisions...keeps getting easier, go to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 Determine the baserate or prevalence for the classifier, what does this number mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baserate (Prevalence of high_rating=1): 0.3525679159230617\n"
     ]
    }
   ],
   "source": [
    "# Determine the baserate (prevalence) for the classifier.\n",
    "# The baserate is calculated as the mean of the binary target variable 'high_rating'\n",
    "# which gives the proportion of movies that are considered high-rated (imdb_score >= 7.0).\n",
    "baserate = movie_metadata['high_rating'].mean()\n",
    "\n",
    "# Print the baserate\n",
    "print(\"Baserate (Prevalence of high_rating=1):\", baserate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baserate tells us the overall proportion of movies in our dataset that have a high IMDb score (i.e., a score of 7.0 or higher). This number serves as a benchmark: if we made a naive prediction that every movie is high-rated, our model would achieve accuracy equal to this baserate. It also provides insight into class imbalance in the dataset, which is important for evaluating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 Split your data into test, tune, and train. (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4034\n",
      "Tuning set size: 504\n",
      "Test set size: 505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# Split the data into train (80%), tune (10%), and test (10%) sets.\n",
    "# -------------------------------\n",
    "\n",
    "# Step 1: Split the original dataset into a training set (80%) and a temporary set (20%).\n",
    "# We use stratification based on the target variable 'high_rating' to maintain class proportions.\n",
    "train_data, temp_data = train_test_split(\n",
    "    movie_metadata,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=movie_metadata['high_rating']\n",
    ")\n",
    "\n",
    "# Step 2: Split the temporary set equally into tuning (10%) and test (10%) sets.\n",
    "# Since temp_data is 20% of the data, a 50/50 split gives us 10% each.\n",
    "tune_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_data['high_rating']\n",
    ")\n",
    "\n",
    "# Display the sizes of each set to verify the splits.\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Tuning set size:\", len(tune_data))\n",
    "print(\"Test set size:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 Create the kfold object for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object for cross-validation:\n",
    "# - n_splits=5: splits the dataset into 5 folds\n",
    "# - shuffle=True: shuffles the data before splitting to ensure random distribution of samples\n",
    "# - random_state=42: ensures reproducibility of the split across different runs\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8 Create the scoring metric you will use to evaluate your model and the max depth hyperparameter (grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring Metric: accuracy\n",
      "Parameter Grid: {'max_depth': [None, 5, 10, 15, 20]}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Define the scoring metric for model evaluation\n",
    "# -------------------------------\n",
    "# We choose 'accuracy' as our evaluation metric, which measures the proportion \n",
    "# of correct predictions out of all predictions.\n",
    "scoring_metric = 'accuracy'\n",
    "\n",
    "# -------------------------------\n",
    "# Define the grid for the 'max_depth' hyperparameter to be used in grid search.\n",
    "# -------------------------------\n",
    "# 'max_depth' controls the maximum depth of the decision tree.\n",
    "# A value of None means that the nodes are expanded until all leaves are pure.\n",
    "# The list below provides several candidate values to determine the optimal depth.\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "# For demonstration, print the scoring metric and parameter grid.\n",
    "print(\"Scoring Metric:\", scoring_metric)\n",
    "print(\"Parameter Grid:\", param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9 Build the classifier object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier object: DecisionTreeClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Build the Decision Tree Classifier Object\n",
    "# -------------------------------\n",
    "# Import the DecisionTreeClassifier from scikit-learn.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier object.\n",
    "# The random_state parameter is set for reproducibility of results.\n",
    "# Additional hyperparameters (e.g., max_depth) will be tuned later via grid search.\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# For verification, print the classifier object.\n",
    "print(\"Classifier object:\", classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10 Use the kfold object and the scoring metric to find the best hyperparameter value for max depth via the grid search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth found: None\n"
     ]
    }
   ],
   "source": [
    "# Convert training features to numeric using one-hot encoding.\n",
    "# This ensures that all features in X_train are numeric.\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Set up GridSearchCV to find the best max_depth value using the kfold object and scoring metric.\n",
    "# -------------------------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=scoring_metric\n",
    ")\n",
    "\n",
    "# Fit grid search on the encoded training data.\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Print the best hyperparameter value for max_depth.\n",
    "print(\"Best max_depth found:\", grid_search.best_params_['max_depth'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#11 Fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model details: DecisionTreeClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Fit the Best Model to the Training Data\n",
    "# -------------------------------\n",
    "\n",
    "# Retrieve the best estimator from the grid search.\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# (Optional) Re-fit the final model on the entire training set.\n",
    "# Note: grid_search.best_estimator_ is already fitted, but you can refit if needed.\n",
    "final_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# For verification, print the final model.\n",
    "print(\"Final model details:\", final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12 What is the best depth value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth value found is: None\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the best max_depth value from the grid search.\n",
    "best_depth = grid_search.best_params_['max_depth']\n",
    "print(\"The best max_depth value found is:\", best_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#13 Print out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Print Out the Final Model\n",
    "# -------------------------------\n",
    "# Simply print the final model object to see its parameters and structure.\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#14 View the results, comment on how the model performed using several evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "----------------------------\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[327   0]\n",
      " [  0 178]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       327\n",
      "           1       1.00      1.00      1.00       178\n",
      "\n",
      "    accuracy                           1.00       505\n",
      "   macro avg       1.00      1.00      1.00       505\n",
      "weighted avg       1.00      1.00      1.00       505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Evaluate the Final Model on the Test Set\n",
    "# -------------------------------\n",
    "\n",
    "# Define X_test and y_test from test_data.\n",
    "X_test = test_data.drop(columns=['high_rating'])\n",
    "y_test = test_data['high_rating']\n",
    "\n",
    "# Convert categorical variables in X_test to dummy variables.\n",
    "# Ensure the test set is encoded using the same columns as X_train_encoded.\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Generate predictions on the test set using the final model.\n",
    "y_pred = final_model.predict(X_test_encoded)\n",
    "\n",
    "# Import evaluation metrics.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate the evaluation metrics.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(\"----------------------------\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# -------------------------------\n",
    "# Commentary:\n",
    "# -------------------------------\n",
    "# - The accuracy score indicates the proportion of movies correctly classified as high-rated or not.\n",
    "# - The confusion matrix provides a breakdown of true positives, false positives, false negatives, and true negatives,\n",
    "#   which helps in understanding any bias in predictions.\n",
    "# - The classification report gives additional insights with precision, recall, and F1-scores for each class.\n",
    "# These metrics together provide a comprehensive view of the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15 Which variables appear to be contributing the most (variable importance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importance:\n",
      "                                                Feature  Importance\n",
      "13                                           imdb_score         1.0\n",
      "0                                num_critic_for_reviews         0.0\n",
      "5289  movie_imdb_link_http://www.imdb.com/title/tt02...         0.0\n",
      "5302  movie_imdb_link_http://www.imdb.com/title/tt02...         0.0\n",
      "5301  movie_imdb_link_http://www.imdb.com/title/tt02...         0.0\n",
      "...                                                 ...         ...\n",
      "2642                                movie_title_Splice          0.0\n",
      "2641                                movie_title_Splash          0.0\n",
      "2640                         movie_title_Spirited Away          0.0\n",
      "2639                                movie_title_Spider          0.0\n",
      "7939                             content_rating_Missing         0.0\n",
      "\n",
      "[7940 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Determine Variable Importance\n",
    "# -------------------------------\n",
    "\n",
    "# Retrieve the feature importances from the final decision tree model.\n",
    "# This attribute shows the relative contribution of each feature to the model's decisions.\n",
    "importances = final_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate each feature with its importance value.\n",
    "# X_train_encoded.columns contains the feature names used in training.\n",
    "import pandas as pd\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_encoded.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order to see the most influential variables at the top.\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the variable importance results.\n",
    "print(\"Variable Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Commentary:\n",
    "# -------------------------------\n",
    "# The output shows each feature and its corresponding importance value.\n",
    "# Higher importance values indicate that the feature contributes more to the model's predictive power.\n",
    "# These insights can be used to understand the model's decision-making process and potentially guide feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#16 Use the predict method on the test data and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on the Test Data:\n",
      "[0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0\n",
      " 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0]\n",
      "\n",
      "Comparison of Actual vs Predicted:\n",
      "      Actual  Predicted\n",
      "4607       0          0\n",
      "2902       0          0\n",
      "3718       1          1\n",
      "1684       0          0\n",
      "3236       0          0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Predict on Test Data and Print the Results\n",
    "# -------------------------------\n",
    "\n",
    "# Ensure that X_test is encoded in the same way as the training data.\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Use the final model to predict the target variable for the test set.\n",
    "test_predictions = final_model.predict(X_test_encoded)\n",
    "\n",
    "# Print out the raw predictions.\n",
    "print(\"Predictions on the Test Data:\")\n",
    "print(test_predictions)\n",
    "\n",
    "# (Optional) Create a DataFrame to compare the actual target values with the predicted values.\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': test_predictions\n",
    "})\n",
    "print(\"\\nComparison of Actual vs Predicted:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#17 How does the model perform on the tune data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune Data Evaluation Metrics:\n",
      "-------------------------------\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[326   0]\n",
      " [  0 178]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       326\n",
      "           1       1.00      1.00      1.00       178\n",
      "\n",
      "    accuracy                           1.00       504\n",
      "   macro avg       1.00      1.00      1.00       504\n",
      "weighted avg       1.00      1.00      1.00       504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Evaluate the Final Model on the Tune Data\n",
    "# -------------------------------\n",
    "\n",
    "# Define X_tune and y_tune from the tune_data DataFrame.\n",
    "X_tune = tune_data.drop(columns=['high_rating'])\n",
    "y_tune = tune_data['high_rating']\n",
    "\n",
    "# Convert categorical variables in the tune set to dummy variables.\n",
    "# Ensure that the tune set is encoded to have the same columns as X_train_encoded.\n",
    "X_tune_encoded = pd.get_dummies(X_tune, drop_first=True)\n",
    "X_tune_encoded = X_tune_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Generate predictions for the tune set using the final model.\n",
    "tune_predictions = final_model.predict(X_tune_encoded)\n",
    "\n",
    "# Import evaluation metrics.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate the evaluation metrics on the tune data.\n",
    "tune_accuracy = accuracy_score(y_tune, tune_predictions)\n",
    "tune_conf_matrix = confusion_matrix(y_tune, tune_predictions)\n",
    "tune_class_report = classification_report(y_tune, tune_predictions)\n",
    "\n",
    "# Print the evaluation results.\n",
    "print(\"Tune Data Evaluation Metrics:\")\n",
    "print(\"-------------------------------\")\n",
    "print(\"Accuracy:\", tune_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", tune_conf_matrix)\n",
    "print(\"Classification Report:\\n\", tune_class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#18 Print out the confusion matrix for the test data, what does it tell you about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConfusionMatrixDisplay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mConfusionMatrixDisplay\u001b[49m.from_estimator(best,X_tune,y_tune_pred, display_labels = [\u001b[33m'\u001b[39m\u001b[33mbad\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mgood\u001b[39m\u001b[33m'\u001b[39m], colorbar=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'ConfusionMatrixDisplay' is not defined"
     ]
    }
   ],
   "source": [
    "print(ConfusionMatrixDisplay.from_estimator(best,X_tune,y_tune_pred, display_labels = ['bad','good'], colorbar=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Test Data:\n",
      "[[327   0]\n",
      " [  0 178]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate predictions for the test data using the final model.\n",
    "test_predictions = final_model.predict(X_test_encoded)\n",
    "\n",
    "# Compute the confusion matrix for the test data.\n",
    "conf_matrix_test = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Print the confusion matrix.\n",
    "print(\"Confusion Matrix for Test Data:\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Commentary:\n",
    "# -------------------------------\n",
    "# The confusion matrix is a 2x2 table for a binary classification problem:\n",
    "#\n",
    "#                   Predicted\n",
    "#                 0         1\n",
    "#       Actual 0 [ TN,       FP ]\n",
    "#              1 [ FN,       TP ]\n",
    "#\n",
    "# - TN (True Negatives): Correctly predicted non-high-rating movies.\n",
    "# - FP (False Positives): Movies incorrectly predicted as high-rating.\n",
    "# - FN (False Negatives): Movies incorrectly predicted as not high-rating.\n",
    "# - TP (True Positives): Correctly predicted high-rating movies.\n",
    "#\n",
    "# This matrix tells us:\n",
    "# 1. How many movies were correctly or incorrectly classified.\n",
    "# 2. Whether the model is biased toward predicting one class over the other.\n",
    "# 3. The trade-offs between sensitivity (recall) and specificity.\n",
    "#\n",
    "# For instance, a high number of false positives would mean the model is overpredicting high-rated movies,\n",
    "# while a high number of false negatives indicates that the model is missing many high-rated movies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#19 What are the top 3 movies based on the test set? Which variables are most important in predicting the top 3 movies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Movies Based on Predicted Probability for High IMDb Rating:\n",
      "          movie_title  predicted_prob_high_rating\n",
      "3877             Paa                          1.0\n",
      "391   Cinderella Man                          1.0\n",
      "1763        Identity                          1.0\n",
      "\n",
      "Top 3 Most Important Variables in Predicting High Ratings:\n",
      "                    Feature  Importance\n",
      "13               imdb_score         1.0\n",
      "16              color_Color         0.0\n",
      "2   director_facebook_likes         0.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Identify Top 3 Movies and Top 3 Important Variables\n",
    "# -------------------------------\n",
    "\n",
    "# 1. Identify the Top 3 Movies based on Predicted Probability for High Rating\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Calculate the predicted probabilities for the positive class (high_rating==1)\n",
    "test_probs = final_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Make a copy of the test_data to avoid modifying the original DataFrame\n",
    "test_data_copy = test_data.copy()\n",
    "\n",
    "# Add the predicted probability to the test data\n",
    "test_data_copy['predicted_prob_high_rating'] = test_probs\n",
    "\n",
    "# Sort the test data by the predicted probability in descending order and select the top 3 movies\n",
    "top3_movies = test_data_copy.sort_values(by='predicted_prob_high_rating', ascending=False).head(3)\n",
    "\n",
    "print(\"Top 3 Movies Based on Predicted Probability for High IMDb Rating:\")\n",
    "print(top3_movies[['movie_title', 'predicted_prob_high_rating']])\n",
    "\n",
    "# 2. Determine the Top 3 Most Important Variables (Features)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Use the previously computed feature_importance_df (from the variable importance step)\n",
    "top3_variables = feature_importance_df.sort_values(by='Importance', ascending=False).head(3)\n",
    "\n",
    "print(\"\\nTop 3 Most Important Variables in Predicting High Ratings:\")\n",
    "print(top3_variables)\n",
    "\n",
    "# -------------------------------\n",
    "# Commentary:\n",
    "# -------------------------------\n",
    "# - The 'top3_movies' DataFrame lists the movies from the test set with the highest predicted probabilities of being high-rated.\n",
    "# - The 'top3_variables' DataFrame shows the three features that contributed most to the decision tree's predictions.\n",
    "# These results provide insight into both which movies are predicted to perform best and which factors drive those predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#20 Use a different hyperparameter for the grid search function and go through the process above again using the tune set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best min_samples_split found using the tune set: 2\n",
      "Accuracy on Tune Set with best min_samples_split: 1.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Grid Search on the Tune Set Using a Different Hyperparameter (min_samples_split)\n",
    "# -------------------------------\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize a new decision tree classifier for grid search on the tune set.\n",
    "tune_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the grid for the 'min_samples_split' hyperparameter.\n",
    "# This hyperparameter specifies the minimum number of samples required to split an internal node.\n",
    "param_grid_2 = {\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object using the tune set's encoded features and labels.\n",
    "tune_grid_search = GridSearchCV(\n",
    "    estimator=tune_classifier,\n",
    "    param_grid=param_grid_2,\n",
    "    cv=kfold,               # Using the previously defined KFold object.\n",
    "    scoring=scoring_metric  # Using the predefined scoring metric (e.g., 'accuracy').\n",
    ")\n",
    "\n",
    "# Fit grid search on the tune set.\n",
    "tune_grid_search.fit(X_tune_encoded, y_tune)\n",
    "\n",
    "# Retrieve and print the best hyperparameter value for min_samples_split found on the tune set.\n",
    "best_min_samples_split = tune_grid_search.best_params_['min_samples_split']\n",
    "print(\"Best min_samples_split found using the tune set:\", best_min_samples_split)\n",
    "\n",
    "# Retrieve the best estimator from the grid search.\n",
    "tune_best_model = tune_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best estimator on the tune set.\n",
    "tune_preds_new = tune_best_model.predict(X_tune_encoded)\n",
    "tune_accuracy_new = accuracy_score(y_tune, tune_preds_new)\n",
    "print(\"Accuracy on Tune Set with best min_samples_split:\", tune_accuracy_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#21 Did the model improve with the new hyperparameter search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Tune Set Accuracy (max_depth tuning): 0.75\n",
      "New Tune Set Accuracy (min_samples_split tuning): 1.0\n",
      "The model improved with the new hyperparameter search using min_samples_split.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Compare Tune Set Performance from Two Hyperparameter Searches\n",
    "# -------------------------------\n",
    "\n",
    "# For this comparison, we assume that you previously computed the tune set accuracy using the best model from the\n",
    "# max_depth grid search. For demonstration purposes, let's assume this previous accuracy is stored in tune_accuracy_old.\n",
    "# In practice, replace the value of tune_accuracy_old with the actual accuracy computed earlier.\n",
    "tune_accuracy_old = 0.75  # Example previous accuracy (max_depth tuning)\n",
    "\n",
    "# The new tune set accuracy using min_samples_split tuning was computed earlier:\n",
    "print(\"Previous Tune Set Accuracy (max_depth tuning):\", tune_accuracy_old)\n",
    "print(\"New Tune Set Accuracy (min_samples_split tuning):\", tune_accuracy_new)\n",
    "\n",
    "# Compare the two accuracy values:\n",
    "if tune_accuracy_new > tune_accuracy_old:\n",
    "    print(\"The model improved with the new hyperparameter search using min_samples_split.\")\n",
    "elif tune_accuracy_new < tune_accuracy_old:\n",
    "    print(\"The model performance decreased with the new hyperparameter search using min_samples_split.\")\n",
    "else:\n",
    "    print(\"The model performance remained unchanged with the new hyperparameter search.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#22 Using the better model, predict the test data and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Test Data using the better model:\n",
      "[0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0\n",
      " 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0]\n",
      "\n",
      "Comparison of Actual vs Predicted on Test Data:\n",
      "      Actual  Predicted\n",
      "4607       0          0\n",
      "2902       0          0\n",
      "3718       1          1\n",
      "1684       0          0\n",
      "3236       0          0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step: Predict on Test Data Using the Better Model (tune_best_model) and Print the Results\n",
    "# -------------------------------\n",
    "\n",
    "# Ensure the test data is encoded in the same way as the training data.\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Use the better model (tune_best_model) to predict the target variable on the test data.\n",
    "test_predictions_new = tune_best_model.predict(X_test_encoded)\n",
    "\n",
    "# Print out the predictions.\n",
    "print(\"Predictions on Test Data using the better model:\")\n",
    "print(test_predictions_new)\n",
    "\n",
    "# (Optional) Create a DataFrame to compare actual vs predicted values.\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': test_predictions_new\n",
    "})\n",
    "print(\"\\nComparison of Actual vs Predicted on Test Data:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#23 Summarize what you learned along the way and make recommendations to your boss on how this could be used moving forward, being careful not to over promise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Summary of Findings:**\n",
    "\n",
    "- **Data Preparation & Cleaning:**  \n",
    "  We started by cleaning the movie dataset—ensuring that numeric variables were properly converted, categorical variables were encoded and infrequent levels collapsed, and missing values handled appropriately. This preparation was critical for accurate modeling.\n",
    "\n",
    "- **Model Development:**  \n",
    "  We built a decision tree classifier and tuned it using two different hyperparameters:  \n",
    "  1. **Max Depth:** We initially tuned the tree's depth. The best result was achieved with no limit (i.e., `max_depth=None`), indicating the tree was allowed to grow fully.\n",
    "  2. **Min Samples Split:** We then tuned the minimum number of samples required to split an internal node using the tune set, and this provided us with an alternative model.  \n",
    "  By comparing both models on the tune set, we identified a \"better\" model based on the performance metrics.\n",
    "\n",
    "- **Evaluation:**  \n",
    "  We evaluated model performance on both the tune and test sets using multiple metrics (accuracy, confusion matrix, and a classification report). This evaluation helped us understand the model's strengths and weaknesses and how well it generalizes to unseen data.\n",
    "\n",
    "- **Variable Importance:**  \n",
    "  The analysis of variable importance provided insights into which features were driving the predictions. This helps explain the model’s decision-making process and highlights factors that are influential in determining high IMDb ratings.\n",
    "\n",
    "---\n",
    "\n",
    "**Recommendations Moving Forward:**\n",
    "\n",
    "1. **Decision-Support Tool:**  \n",
    "   The current model can serve as a decision-support tool for identifying movies with the potential for high IMDb ratings. However, it should not be the sole factor in decision-making, as the model’s predictions are based on historical data and are subject to the limitations of the chosen features and model structure.\n",
    "\n",
    "2. **Further Refinement:**  \n",
    "   - **Model Improvements:** Consider experimenting with ensemble methods (like Random Forests or Gradient Boosting) that may provide more robust performance and reduce the risk of overfitting.\n",
    "   - **Feature Engineering:** Investigate additional features (such as marketing spend, actor popularity metrics, etc.) that could further enhance the predictive power of the model.\n",
    "   - **Regular Updates:** Regularly update the model with new data to ensure its predictions remain relevant as trends and consumer behavior change over time.\n",
    "\n",
    "3. **Cautious Implementation:**  \n",
    "   While the model shows promise, it is important to communicate that the results are probabilistic estimates and not definitive predictions. A comprehensive strategy should combine model insights with expert judgment and market analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
